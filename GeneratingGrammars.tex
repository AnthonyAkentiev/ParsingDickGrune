\chapter{Грамматики как способ генерации текста}
%(бывшая Grammars as a Generating Devices)

%23.04.06
\section{Язык как бесконечное множество}
	Как и в повседневной жизни, в компьютерном мире \textit{грамматика} служит для \textit{описания} \textit{языка}. Тем не менее, компьютерный инженер и простой человек понимают эти три выделенных слова по-разному. Мы должны подробно изучить их, для того, чтобы дать им четкое определение. Итак...\\

\subsection{Язык}
	Для большей части человечества язык является первым и самым главным средством общения, используемым подсознательно. Коммуникация осуществляется благодаря сообщениям, передаваеым через воздух или бумагу. При более детальном рассмотрении, языковые сообщения ("`высказывания"') делятся на предложения, которые в свою очередь делятся на слова, состоящие из последовательностей символов (при коммуникации в письменной форме). Язык может влиять на эти три слоя. Написание символов может лишь немного различаться (сравните английский язык и ирландский), а может различаться очень значительно (сравните английский и китайский). Слова могут сильно разниться даже в очень родственных языках: одни назовут лошадь \textit{un cheval}, а другие \textit{ein Pferd}. Различия в структуре предложений часто недооценивают. Даже в немецком языке бывают фразы, типичные для Шекспира: "`Ik geloof je niet"', т.е. "`I believe you not"'\footnote{Автор имеет в виду нетипичный порядок слов - Прим.пер.}, а в таком далёком от английского венгерском языке фраза вообще не имеет близких аналогов: "`P$\acute{e}$nzem van"' переводилось бы как "`Money-my is"', что вовсе не схоже с правильным выражением "`I have money"'.\\
	Компьютерный инженер\footnote{Далее просто - инженер - Прим.пер.} должен взглянуть на это более абстрактно. Да, язык имеет предложения, они имеют структуру. Для нас не имеет значения, что передаёт предложение, но мы имеем возможность получить(или \textit{вывести}) информацию из его структуры, которую и будем считать \textit{значением} предложения. Да, предложения состоят из слов, которые назовём \textit{токенами}, а каждое слово (возможно) несёт частицу информации, которая влияет на предложение в целом. Но нет, слова не могут быть разделены далее. Инженера не трогает это ограничение, так как его любовь к многоуровневым проблемам и решениям даёт ему новый заряд энергии, который подсказывает, что если слова образуют структуру, то они определённо образуют предложение на языке, в котором буквы являются токенами.\\
	Человек, использующий формальную лингвистику (чтобы не спутать с лингвистом обыкновенным, назовём его просто - "`формальный-лингвист"'), может иметь свой взгляд: язык это "`множество"' предложений; каждое предложение это "`последовательность"' "`символов"', и не важно, есть ли у него значение или определённая структура. Да и вообще, принадлежит ли предложение к языку, нашего лингвиста тоже не волнует. Единственное свойство символов заключается в том, что они различны; любой язык включает некоторое конечное количество различных символов (\textit{алфавит}). Просто для примера покажем, что мы записываем символы как $a,b,c \cdots$ или  
%вставить символы...
как $\cdots$, и можем продолжать так до тех пор, пока хватет обозначений. Слово "`последовательность"' означает, что символы в каждом из предложений идут в определённом порядке, и мы не будем их произвольно перемещать. "`Множество"' означает неупорядоченный набор, не имеющий дубликатов. Множество можно перечислить (обозначить) записью в фигурных скобках каждого символа, входящего в него. Всё вышеприведённое означает, что $\{ a,b,ab,ba\} $,как и $\{ a,aa,aaa,aaaa, \cdots\} $ формальный-лингвист будет считать предложениями. Последняя запись имеет недостаток, который мы рассмотрим позже. В соответствии со взглядами инженера на взаимосвязи предложение/слово и слово/буква, формальный-лингвист также называет предложение \textit{словом} и может, например, сказать "`слово $ab$ присутствует в языке $\{ a,b,ab,ba\} $"'.\\
	Давайте рассмотрим следствия этих небольших, но грандиозных идей.\\
	Для инженера, язык является бесконечно большим множеством предложений, каждое из которых состоит из токенов так, что они образуют структуру. Токены и структура взаимодействуют для отображения семантики предложения, т.е. его смысла, если желаете. И структура, и семантика - это новшества, которые не были представлены в формальной модели, хотя её назначение - управлять ими. Для инженера $3+4*5$ является предложением "`арифметического"' языка с одиночными цифрами ("`одиночные цифры"' введены для того, чтобы не появилось бесконечной последовательности символов), его структура может быть отображена, допустим, с помощью добавления скобок: $(3+(4*5))$. Семантическое значение предложения равно $23$\footnote{Обычно говорят, что семантика предложения равна ... - Прим.пер.}.\\
	Для настоящего лингвиста, который, мы не можем не согласиться, имеет более приземлённые взгляды, чем два предыдущих персонажа, язык является бесконечным множеством взаимосвязанных предложений. Каждое предложение состоит из слов, которые имеют смысл в реальном мире. Структура и сами слова задают предложению смысл, который оно передаёт. Слова, аналогично, имеют структуру и состоят из букв; буквы тесно взаимодействуют со структурой, чтобы дать слову смысловое значение. Основное направление лингвистики заключается в том, что на семантику, а также на связь с реальным миром, как и на отношения предложение/слово и слово/буква, делается очень сильное ударение. "`Шайба летит быстро"' - это предложение, "`Шайба спит красный"' - это бессмыслица.\\
	Формальный-лингвист придерживается своих взглядов на язык потому что он желает понять основные свойства языков; инженер придерживается своих, так как ему нужен четкий, простой и однозначный способ описания объектов в компьютере, а также для коммуникации с компьютером, который, в отличии от человека, является очень строгим партнёром. Лингивст же верит в свою точку зрения, так как она даёт ему сильную формальную связь с хаотичным и бесконечно сложным объектом: человеческим языком.\\

\subsection{Грамматики}
	Любой, кто изучал иностранный язык, знает, что словарь\footnote{Общепринятый перевод данного термина - грамматика - Прим.пер.} это книга правил и примеров, описывающая язык и помогающая изучить его. В хорошей книге по иностранному языку чувствуется большое отличие между уровнем предложение/слово, которое часто называют \textit{синтаксисом}, и уровнем слово/буква, которое называют непосредственно \textit{словарём}. Синтаксис состоит из правил вида "` \textit{pour que} следует за подлежащим, а \textit{parce que} нет"'; словарь состоит из правил вида "`Множественное число существительного образуется добавлением окончания -s, кроме случаев, когда слово оканчивается на -s, -sh, -o, -ch, -x. Тогда добавляется окончание -es"'.\\
%24.04.06
	Мы опустим на некоторое время обсуждение мыслей инженера и поговорим о формальном-лингвисте. Его взгляд с одной стороны очень абстрактен, а с другой очень походит на приведённый выше: грамматика - любое конкретное, конечное и полное описание языка, т.е. набора предложений. Это, с некоторыми оговорками, по сути, и является школьным словарём. Понятно, что такое определение весьма абстрактно, а значит оно относительно бессильное. Это весьма похоже на такое определение словаря: "`набор предложений, изданный примерно в 1987 году"'. Да, это определяет множество, но мы не имеем возможности создания такого множества или проверки, включает ли оно какое-либо определённое предложение. В частности, в вышеприведённом примере слово "`примерно"' не испугает формального-лингвиста, но есть более близкие примеры: "`наидлиннейшая последовательность семёрок в десятичной записи числа $\pi$"' описывает язык, в котором имеется лишь одно слово (состоящее лишь из семёрок). Такое определение будет конкретным, конечным и полным. Беда в том, что никто не сможет найти такое слово. Представьте, что кто-то отыскал последовательность из сотни семёрок, после изучения миллионов цифр после запятой. Он не может быть уверенным, что далее эта последовательность не продолжится, но самое интересное, что он даже не смог бы заранее знать существует ли наидлиннейшая последовательность вообще. Возможно, при дальнейшем исследовании он бы встретил многочисленные и более продолжительные последовательности семёрок. Теория десятичного разложения числа $\pi$ смогла бы ответить на эти вопросы, но её просто не существует.\\
	По этой, а также другим, причинам, формальному-лингвисту пришлось бы изменить свой подход на более конструктивный, например на концепцию, называемую порождающей грамматикой.\\
\textit{Порождающей грамматикой} называют механизм построения предложений языка. Из этого следует, что следуя инструкциям грамматики можно построить за конечное число действий \textbf{любое} предложение языка, и никакое другое. Это не означает, в свою очередь, что инструкции укажут, \textit{как} построить \textbf{конкретное} предложение, они могут только убедить нас, что это возможно. Инструкции могут иметь различные виды, некоторые из которых оказываются удобнее других.\\
	Инженер придерживается абсолютно таких же взглядов, но часто имеет дополнительное условие: инструкция должна показывать, как предложение должно быть построено.\\
	
\subsection{Сложности}
	Из вышеприведённого определения языка, как (возможно) бесконечного множества последовательностей из символов, а также грамматики как инструкции к построению, вытекает два непростых вопроса:	
\begin{enumerate}
	\item Как может конечно число инструкций описать бесконечное число последовательностей?
	\item Если предложение является простой последовательностью без структуры, и если значение (смысл) предложения определяется (выводится), помимо всего прочего, из его структуры, то как мы сможем определить его значение?
\end{enumerate}

Эти вопросы приводят к длинным и весьма сложным ответам, но такие ответы существуют! Для начала обратим внимание на первый вопрос, а затем посвятим оставшуюся часть книги ответу на второй.\\

\sss{Бесконечные множества из конечных описаний}
	Вообще, нет ничего необычного в получении бесконечного множества из конечно набора описаний. Например, "`множество всех положительных чисел"' это очень краткое и конечное описание бесконечного множества. Но всё-равно с этим не всё так просто, так что давайте перефразируем свой вопрос: "`могут ли все языки быть заданы конечными описаниями?"'. Конечно, ответ  отрицателен, но доказать это не легко. Тем не менее, доказательство очень интересное и давно известное, так что трудно будет не привести хотя бы его часть.\\

\sss{Описания можно перечислить}
	Доказательство основано на двух наблюдениях и одной маленькой хитрости. Первое наблюдение заключается в том, что описания могут быть перечислены и пронумерованы. Давайте поступим так. Сначала, возьмём все описания единичного размера, то есть состощие из одной буквы, а затем отсторитируем их в алфавитном порядке. Это будет началом нашего списка. В зависимости от того, что мы называем описанием, может существовать нуль или 27 (все буквы английского языка плюс пробел) или 128 (все ASCII символы), или какое другое число таких описаний. Их количество не повлияет на наше дальнейшее обсуждение.\\
	Далее мы отсортируем все описания с длиной 2 и поместим их в наш список, и так далее для остальных описаний. Теперь каждому описанию присвоен определённый номер. Для примера, наше описание "`множество всех положительных чисел"' будет иметь длину 34 символа (без кавычек). Чтобы найти его расположение в списке, мы должны подсчитать количество описаний с длиной меньше 34. Назовёт это число \textit{L}. Затем нужно сгенерировать все возможные описания длины 34, отсортировать их и определить позицию нашего описания (пусть это будет \textit{P}). Последним шагом является сложение двух полученных чисел \textit{L} и \textit{P}. Это будет, конечно же, огромное число, но этим самым мы докажем, что описание находится в определённой позиции, см. рис.2.1.\\

%25.04.06
Обратите внимание на два момента. Первый заключается в том, что простое перечисление всех описаний без учета их длины не даст положительного результата, так как существует бесконечное количество описаний, начинающихся с "`а"', и мы никогда не сможем получить описание, начинающееся со следующей буквы. Второй заключается в том, что нам и не нужно этого делать. Это лишь теоретический эксперимент, так сказать модель того, что в реальной жизни получить не получится.\\
	Конечно, список будет содержать огромное число неважных для нас описаний. Главное - убедиться, что все те, которые нам нужны в нём есть, доказательство чего приведено чуть выше.\\
	
\sss{Языки и бесконечные битовые строки}
Мы узнали, что слова (предложения) языка состоят из конечного множества символов; такое множество называют \textit{алфавитом}. Будем считать, что символы алфавита упорядочены, а значит слова тоже могут быть упорядочены. Обозначим алфавит греческой буквой $\Sigma$.\\
	Самый простой язык, использующий алфавит $\Sigma$, это язык, который образуется путём комбинирования букв алфавита. Для алфавита $\Sigma= \{ a,b \}$ получим язык $\{ \ ,a,b,aa,ab,ba,bb,aaa,\cdots \}$. По далее рассматриваемым причинам, будем называть такой язык $\Sigma^{*}$.\\
	Мы начали обозначение множества $\Sigma^{*}$ с \textit{пустого слова}, то есть слова, содержащего нулевое число символов $a$ и нулевое число символов $b$. Нет причин его игнорировать, так что для б$\acute{o}$льшей ясности будем обозначать его, независимо от используемого алфавита, так - $\varepsilon$. Так что, $\Sigma^{*}=\{ \epsilon ,a,b,aa,ab,ba,bb,aaa,\cdots \}$. В некоторых естественных языках, форма настоящего времени вспомогательного глагола (как "`to be"' в английском языке) есть пустое слово. Например, так обстоит дело и в русском языке ("`Я студент"').\\
	Так как символы в алфавите  $\Sigma$ отсортированы, мы можем перечислить слова языка  $\Sigma^{*}$ используя технику, приведённую в предыдущем абзаце: сначала сортируем слова размером в одну букву, затем - в две, и так далее. Такой порядок имело и наше обозначение множества слов языка  $\Sigma^{*}$.\\
	Язык  $\Sigma^{*}$ обладает интересной особенностью: все языки, использующие алфавит  $\Sigma$ являются его подмножеством. Это означает, что имея любой другой язык $L$ мы можем пройтись по списку слов $\Sigma^{*}$ и отметить слова из $L$. Тем самым мы выделим все слова языка $L$, так как $\Sigma^{*}$ содержит любое возможное слово из $\Sigma$.\\
	Пусть дано следующее определение языка $L$: "`множество всех слов, содержащих больше букв $a$, чем букв $b$"'. $L=\{a,aa,aab,aba,baa, \cdots \}$. Начало нашего списка (с отметками) будет выглядеть так:\\
\begin{figure}[ht]
\DHSp
\label{fig_marked}
\begin{tabular}{llr}
\GrT{$$} & 																$\varepsilon$\\
\GrT{$\surd$} & 												$a$\\
\GrT{$$} & 												$b$\\
\GrT{$\surd$} & 												$aa$\\
\GrT{$$} & 												$ab$\\
\GrT{$$} & 												$ba$\\
\GrT{$$} & 												$bb$\\
\GrT{$\surd$} & 												$aaa$\\
\GrT{$\surd$} & 												$aab$\\
\GrT{$\surd$} & 												$aba$\\
\GrT{$$} & 												$abb$\\
\GrT{$\surd$} & 												$baa$\\
\GrT{$$} & 												$bab$\\
\GrT{$$} & 												$bba$\\
\GrT{$$} & 												$bbb$\\
\GrT{$\surd$} & 												$aaaa$\\
\GrT{$\cdots$} & 												$\cdots$\\
\end{tabular}
\caption{Начало списка описаний}
\end{figure}

Для полноценного описания языка такого списка отметок и пропусков будет достаточно. Для простоты будем использовать 0 для элемента без отметки, а отметку будем обозначать 1, как будто это биты информации. Теперь мы можем описать язык так: $L=0101000111010001 \cdots$, а $\Sigma^{*}=111111111111 \cdots$. Нужно заметить что все изложенное остаётся в силе для \textit{любого} языка, будь то формальный язык (например, $L$), язык программирования (например, Pacal) или естественный язык (например, английский). Для английского языка отметки будут встречаться очень редко (вряд ли любая последовательность символов является английским словом).

%29.04.06
\sss{Диагонализация}
В предыдущем примере мы называли бесконечную битовую строку $0101000111010001 \cdots$ множеством всех слов, содержащих большее количество символов $a$, чем $b$. В том же духе мы можем найти большое количество подобных описаний для этой строки; некоторые из них не "`построят"' язык, а значит мы можем описать ими бесконечно длинную последовательность битов. Так как все описания могут быть помещены в один пронумерованный список, мы получим следующую картину:
\begin{figure}[ht]
\DHSp
\label{fig_desc}
\begin{tabular}{llr}
\GrT{$\textit{Description}$} & 									&$\ \textit{Language}$\\
\GrT{$Description\ \# 1$} & 										&$000000100 \cdots$\\
\GrT{$Description\ \# 2$} & 										&$110010001 \cdots$\\
\GrT{$Description\ \# 3$} & 										&$011011010 \cdots$\\
\GrT{$Description\ \# 4$} & 										&$110011010 \cdots$\\
\GrT{$Description\ \# 5$} & 										&$100000011 \cdots$\\
\GrT{$Description\ \# 6$} & 										&$111011011 \cdots$\\
\GrT{$\cdots$} & 														&$\ \cdots$\\
\end{tabular}
%\caption{}
\end{figure}
\\
Слева расположены все описания, а справа расположены все языки, описанные ими. Теперь видно, что существует большое количество языков, не попавших в правую часть списка: в отличии от левой части (описаний), правая не закончена. Мы докажем это с помощью диагонализации Кэнтора (Cantor).\\
	Представим язык $C=100110 \cdots$, каждый \textit{n}-ный бит которого не равен \textit{n}-ному биту языка, заданного \textit{n}-ным описанием. Первый бит язка $C$ равен 1, так как первый бит в первом языке равен 0; второй бит $C$ равен 0, так как второй бит второго языка равен 1, и так далее. Язык $C$ построится последовательным движением по диагонали с СЗ на ЮВ и копированием инвертированных битов.\\
	Но языка $C$ не может быть в списке! Он не может находиться в первой позиции, так как его первый бит не такой, как должен быть. Он вообще не может быть на \textit{n}-ной строке, так как \textit{n}-ный бит отличается от того, который должен быть там по определению.\\
	
	Поэтому, основываясь на факте, что мы включили в список все возможные конечные описания, мы нашли такое описание, которое туда не попало. Более того, любая "`разбитая"' диагональ даст нам такой язык, где "`разбитой"' называют диагональ следующего вида:
\begin{figure}[h]
	\centering
		\includegraphics[width=7cm]{figs/gener/p22}	
\end{figure}
По сути, для каждого языка из списка существует бесконечное число языков, не попавших в него. Теперь нам понятно это утверждение, так что мы не будем его доказывать.\\
	Техника диагонализации более детально рассматривается во многих книгах, например Рейард-Смит (Rayward-Smith) [Books 1983, стр 5-6] или Хопкрофт и Ульман (Hopcroft and Ulman) [Books 1979, стр 6-9].\\
	
%29.04.06
\sss{Заключения}
Во-первых, мы познали силу работы с языками, как с формальными объектами. Но всё вышеприведённое нужно хорошенько "`просеять"' и найти подходящие доказательства (хотя бы одно утверждение должно быть доказано - почему сам язык $C$ не входит в список описаний). Это поможет открыть другие особенности такого подхода.\\
	Во-вторых, мы узнали, что можно описать лишь небольшое подмножество (очень небольшое) всех возможных языков: существует бесконечное число языков, до которых мы никогда не дойдём.\\
	Дальше, мы доказали, что существует бесконечное число описаний и бесконечное число языков, но такие бесконечности вовсе не эквивалентные: вторых языков больше, чем первых. По Кэнтору, эти бесконечности обозначаются $\aleph_0$  и $\aleph_1$, а наше доказательство является лишь адаптацией аналогичного доказательства Кэнтора: $\aleph_0 < \aleph_1$.\\
	
\subsection{Конечное описание языка}
Начинать с простого объекта и описать правила построения из него всех последующих объектов - лучший способ построить множество. Утверждение "`два - чётное число, а сумма двух чётных чисел - тоже чётное число"' построит множество всех чётных чисел. Педанты добавят "`... и никакое другое число не является чётным"', но мы опустим это.\\
	Представьте, что мы желаем построить множество всех наборов имён (вроде "`Том, Дик и Гарри"'), в которых все имена, кроме последнего, отделяются запятыми. Не будем запрещать дубликаты: "`Гарри, Гарри и Дик"' вполне нас устроит. Конечно, это не полноценные предложения, но мы все-равно будем называть их \textit{предложениями}, так как они являются таковыми в нашем скромном языке. Вот простой пример описания построения языка:\\
	
\begin{enumerate}
	\item Том - имя, Дик -имя, Гарри - имя;
	\item имя это предложение;
	\item предложение, после которого следует запятая и имя - вновь предложение;
	\item в конце заменить "`,имя"' на "`и имя"'.
\end{enumerate}
	
	Вроде бы все в порядке, но если присмотреться к 4му шагу, то станет понятно, что он ошибочный. Вообще, предложения не будут оканчиваться на "`,имя"', они будут оканчиваться (например) на "`,Дик"'. "`Имя"' это просто символ, который заменяет реальное имя; такой символ не может встретиться в реальном предложении, и должен быть в конце концов заменён на имя из шага 1. Аналогично, "`предложение"' это символ, который заменяет все настоящие предложения. Получается, что мы имеем два вида символов: реальные символы, которые встречаются в конечном предложении (вроде "`Том"', "`Дик"', запятой и слова "`и"') и "`промежуточные"' символы (вроде "`предложение"' и "`имя"'), которые не встречаются в конечном предложении. Первые относятся к словам или \textit{токенам}, о которых говорилось выше; их техническое название - \textit{терминальные символы}. Вторая группа называется, соответственно, \textit{нетерминалами}. Чтобы отличать их, будем записывать терминалы строчными буквами, а нетерминалы прописными.\\
	%01/05/06
	Чтобы лучше увидеть производящий характер нашей последовательности шагов, заменим правила вида "`X это Y"' на "`Y может быть заменён на X"': если "`том"' это экземпляр Имени, то заменим все нетерминалы Имя на "`том"'. Преобразуем шаги:
		
\begin{enumerate}
	\item Имя может быть заменено на "`том"'\\
	Имя может быть заменено на "`дик"'\\
	Имя может быть заменено на "`гарри"'
	\item Предложение может быть заменено на Имя
	\item Предложение может быть заменено на Предложение, Имя
	\item "`,Имя"' в конце предложения должно быть заменено на "`и Имя"', перед тем, как само Имя будет заменено. \
	\item Предложение построено, если оно не содержит ни одного нетерминала
	\item Начинать замену нужно с Предложения
\end{enumerate}

Шаги 1-4 описывают возможные замены, но 5 и 6 отличаются от них. Пятый шаг не относится лишь к нашей \textit{грамматике}. Он всегда присутствует и является одним из главных правил нашей игры. Шаг 6 уточняет, откуда нужно начинать построение (или \textit{вывод}\footnote{Общепринятый перевод слова derivation - Прим.пер.}). Такой символ называют \textit{начальным символом} и он должен иметься в любой грамматике.\\
	Шаг 4 выглядит довольно странно. Большинство \textit{правил\footnote{Общепринятый перевод слова rule - Прим.пер.}} рекомендует ("`может быть заменено"'), а не требует ("`должно быть заменено"'). Остальные правила работают с выводом, но нас интересует, как мы можем использовать вывод для определения конца предложения. Если мы используем маркер окончания, то есть терминал, который не может быть использован нигде кроме замены "`,Имя"' на "`и Имя"', то мы должны добавить утверждение, что предложение не окончено, пока такая замена не произойдёт. Для краткости будем записывать $\rightarrow$ вместо "`может быть заменён"'. Всё, что идёт слева от такого значка будет называться левой частью, а всё, что идёт справа - правой. Это приводит нас к рисунку 2.2.\\

\begin{figure}[ht]
\DHSp
\label{fig2.2}
\begin{tabular}{llcl}%\hspace{3.5cm}
\GrT{$0.$} & $Name$ &   $\rightarrow$ & $tom$\\
\GrT{$$} & $Name$ &   $\rightarrow$ & $dick$\\
\GrT{$$} & $Name$ &   $\rightarrow$ & $harry$\\

\GrT{$1.$} & $Sentence$ &   $\rightarrow$ & $Name$\\
\GrT{$$} & $Sentence$ &   $\rightarrow$ & $List\ End$\\

\GrT{$2.$} & $List$ &   $\rightarrow$ & $Name$\\
\GrT{$$} & $List$ &   $\rightarrow$ & $List, Name$\\

\GrT{$3.$} & $, Name End$ &   $\rightarrow$ & $and Name$\\
\GrT{$4.$} & начальным символом является $Sentence$\\
\end{tabular}
\caption{Грамматика для вывода строк языка "`т,д и г"'}
\end{figure}

%05.05.2006
Это простая и относительно точная форма описания правил, в которой правила записываются так, как мы их и представляем (последовательно): начнём с начального символа и будем производить замены, пока не останется нетерминалов. 

\section{Формальные грамматики}
	Вышеприведённая форма, основанная на замене по правилам, достаточно мощна, чтобы служить базисом для формальной грамматики. У математиков давно сущесвует похожее понятие - "`перезаписывающий процесс"', но особая форма грамматики с рис.2.2 была впервые изучена Хомским[Misc 1959]. Проведённый им анализ дал толчок для многих последующих исследований формальных грамматик, разборщиков, изучению особенностей лингвистики и построения большей части компиляторов.\\
	Так как формальные грамматики являются "`владениями"' математиков, работа с ними ведётся с помощью специальной нотации, которая может показаться сложной для новичка. Для того, чтобы окунуться в мир формальной лингвистики, необходимо дать формальное определение грамматики, а затем объяснить, как можно описать грамматику (подобную рисунку 2.2). Формализм использован здесь лишь для доказательств, но не для понимания принципов.\\


%06/05/06
\begin{definition}
\textit{Порождающей грамматикой} называют такой кортеж ($V_N,V_T,R,S$), в котором (1) $V_N$ и $V_T$ - бесконечные множества символов, (2) $V_N\cap V_T=\oslash$, (3) $R$ - множество пар $(P,Q)$, таких, что (3а) $P\in (V_N \cup V_T)^+$ и (3б) $Q \in (V_N \cup V_T)^*$ и (4) $S \in V_N$
\end{definition}
	Кортеж из 4х элементов это объект, содержащий 4 части. В данном случае это (по порядку): нетерминалы, терминалы, правила и начальный символ. Настоящее определение не включает этого. Множество нетерминалов обозначается так - $V_N$, а множество терминалов - $V_T$. Для нашей грамматики имеем:\\
	
\begin{figure}[h]
\HSp
\begin{tabular}{lcl}
\label{figvtvn}
$V_N$ & = & $\left\{ Name,Sentence,List,End\right\}$ \\
$V_T$ & = & $\left\{ tom,dick,harry,\ ','\ ,and \right\}$
\end{tabular}
\end{figure}
(Обратите внимание на запятую во множестве терминальных символов).\\
	Пересечение множеств $V_N$ и $V_T$ (2) должно быть пусто, то есть терминалы и нетерминалы не должны иметь одинаковых символов.\\
	$R$ это множество всех правил (3), а $P$ и $Q$ это левые и правые стороны соответсвенно. Каждое $P$ должно содержать последовательности из одного (или более) нетерминалов или терминалов, а каждое $Q$ должно содержать последовательности из нуля (или более) нетерминалов или терминалов. Для нашей грамматики:\\
\begin{figure}[h]
\HSp
\begin{tabular}{lcl}
\label{figvtvn2}
$R = \{ (Name,tom),(Name,dick),(Name,harry),$\\
$(Sentence,Name),(Sentence,List\ End),$\\
$(List,Name),(List,List \ ','\ ),(\ ','\ Name\ End, and\ Name)\}$ \\
\end{tabular}
\end{figure}

%13.05.06:
Вновь обратите внимание на запятые.\\
	Начальный символ $S$ должен входить в множество $V_N$, то есть он должен являться нетерминалом:
	$$S\ =\ Sentence$$
	На этом наше путешествие в формальную лингвистику оканчивается, но читатель может не сомневаться - очень многое осталось без внимания. Хорошее и доступное введение написано Ривецом (Revesz) [Books 1985].\\
	
%проверить:
% объяснить PS
\subsection{Образование предложений с помощью формальной грамматики}
Грамматика с рис. 2.2 является \textit{фразовой структурной грамматикой\footnote{Далее в этом разделе просто называемой "`грамматикой"' - Прим. пер.}} для нашего языка (обычно кратко называемая -  PS грамматикой\footnote{От Phase Structure - Прим. пер.}). Существует и более компактная запись, в которой различные правые части c одинаковой левой частью сгруппированы вместе и отделены вертикальной чертой |. Этот символ является простой условностью (такой же, как и стрелка $\rightarrow$), и может быть озвучен словом "`или"'. Правые части, отделённые таким символом называют \textit{альтернативами}. Теперь наша грамматика выглядит короче:
\begin{figure}[h]
\HSp
\begin{tabular}{lrcl}
\label{figgram}
$0.$ & $Name$ & $\rightarrow$ & $tom\ |\ dick\ |\ harry$\\
$1.$ & $Sentence_S$ & $\rightarrow$ & $Name\ |\ List\ End$\\
$2.$ & $List$ & $\rightarrow$ & $Name\ |\ Name \ ,\ List$\\
$3.$ & $,\ Name\ End$ & $\rightarrow$ & $and\ Name$\\
\end{tabular}
\end{figure}
где нетерминал с индексом $S$ является начальным символом. (Индекс $S$ обозначает символ, а не правило.)\\	
	Теперь давайте выведем наше предложение из этой грамматики, используя лишь вышеприведённые правила. Мы получим следующие существующие формы для $Sentence$:\\
\begin{table}[h]
\label{figderive1}
\begin{tabular}{lll}
\GrT{Промежуточная форма}& \GrT{Использованное правило}& \GrT{Примечание}\\
\GrT{$Sentence$}& \GrT{-}& \GrT{Начальный символ}\\
\GrT{$List\ End$}& \GrT{$Sentence\ \rightarrow \ List\ End$}& \GrT{Правило 1}\\
\GrT{$Name\ ,\ List\ End$}& \GrT{$List\ \rightarrow \ Name\ ,\ List$}& \GrT{Правило 2}\\
\GrT{$Name\ ,\ Name\ ,\ List\ End$}& \GrT{$List\ \rightarrow \ Name\ ,\ List$}& \GrT{Правило 2}\\
\GrT{$Name\ ,\ Name\ ,\ Name\ End$}& \GrT{$List\ \rightarrow \ Name$}& \GrT{Правило 2}\\
\GrT{$Name\ ,\ Name\ and\ Name$}& \GrT{$,\ Name\ End\ \rightarrow \ and\ Name$}& \GrT{Правило 3}\\
\GrT{$tom\ ,\ dick\ and\ harry$}& \GrT{-}& \GrT{Правило 0, трижды}\\
\end{tabular}
\end{table}

Промежуточные формы называются \textit{сентенциональными}; если сентенциональная форма не содержит нетерминалов, она называется \textit{предложением} и принадлежит к сгенерированному языку. Переходы от одной строки к другой называют \textit{шагами продукции}, а такие правила, по понятным причинам, называют просто \textit{продукциями}.\\
	Процесс вывода можно отобразить графически с помощью рисования соединительных линий между соответствующими символами, как показано на рис. 2.3. Такой рисунок называют \textit{графом вывода}\footnote{Иногда можно называть деревом вывода - Прим. пер.} или \textit{синтаксическим графом}, так как он отражает синтаксическую структуру (согласно данной грамматики) конечного предложения. Видно, что наш синтаксический граф "`расширяется вниз"'. Кроме того, можно заметить крестообразные конструкции, которые ведут к перестановке некоторых символов.\\
\begin{figure}[h]
	\centering
		\includegraphics[width=10cm]{figs/gener/fig23}		
	\caption{Граф вывода для предложения}
\end{figure}
	У нас нет возможности явно сгенерировать предложение $tom,\ dick,\ harry$, так как любая попытка произвести более одного имени приведёт к символу $End$, а чтобы от него избавиться (мы должны сделать это, так как это нетерминал) необходимо применить правило 3, которое выведет $and$. Мы видим, к нашему удивлению, что мы таки реализовали требование "`должен заменить"' с помощью системы, которая использует только "`может быть заменён"'. Присмотревшись получше, видно, что мы объеденили "`может быть заменён"' с ещё одним требованием - "`не должен быть нетерминалом"'.\\
	Помимо нашего предложения, эта грамматика, конечно же, способна выводить множество других предложений; вот примеры:\\
\begin{center}
\begin{table}[h]
\label{figderexample1}
\begin{tabular}{l}
\GrT{$harry\ and\ tom$}\\
\GrT{$harry$}\\
\GrT{$tom\, \ tom\, \ tom\, and\ tom$}\\
\end{tabular}
\end{table}
\end{center}
... и множество других. Авантюрный способ сгенерировать предложение без $and$ приведёт нас к следующей сентенциональной форме:\\
\begin{center}
\begin{table}[h]
\label{figderexample2}
\begin{tabular}{l}
\GrT{$tom,\ dick,\ harry\ End$}\\
\end{tabular}
\end{table}
\end{center}
Такие формы не будут являться предложениями. Они называются \textit{тупиками}. Обратите внимание, что продукции нельзя применить в обратном порядке.\\

%p 17
% 14.05.06
\subsection{Описательная мощность формальных грамматик}
Главным свойством грамматики является наличие правил вывода (продукций), которые могут быть использованы для замены частей сентенциональных форм (создание предложения) и существование начального символа, который является материнским для всех сентенциональных форм. В продукциях мы имеем как терминалы, так и нетерминалы; конечные предложения содержат лишь терминалы. Вот и всё. Остальное остаётся на совести создателя грамматики.\\
	Это очень экономное описание, поэтому возникает вопрос: самодостаточна ли такая система? Странно, но доказно, что все другие известные способы создания множеств, известные человечеству, имеют такую же (или даже меньшую) мощность\footnote{Рассматривается описательная мощность - Прим. пер.}. Одним из очевидных способов создания множества является написание программы, генерирующей его, но также доказано, что любое множество, сгенерированное программой, может быть сгенерированно с помощью грамматики. Существует множество различных неявных методов, но ни один из них не является более мощным. С другой стороны, мы не имеем доказательства, что такой, более мощный, метод не существует. Но, если учесть, что все подходы останавливались на том же самом месте, очень маловероятно\footnote{Пол Витани (Paul Vitany) объяснил, что даже когда учёные говорят "`очень маловероятно"', они никогда не поставят свою годовую зарплату на это}, что такой метод когда-нибудь будет найден. См. Ривец (Revesz) [Books 1985, стр. 100-102].\\
	Чтобы ещё раз показать описательную мощь грамматик, давайте рассмотрим следующий пример, в котором моделируется движение Манхэттонской черепашки. \textit{Манхэттонской черепашкой} называют существо, летающее на самолёте строго на север, юг, запад или восток на еденичное расстояние за раз. Грамматика с рис. 2.4 выводит все пути, которые в результате приведут черепашку обратно в начальный пункт отправления.\\
\begin{figure}[h]
\HSp
\begin{tabular}{lrcl}
\label{figmanhturtle}
$1.$ & $Move_S$ & $\rightarrow$ & $north\ Move\ south\ |\ east\ Move\ west\ |\ \varepsilon$\\
$2.$ & $north east$ & $\rightarrow$ & $east\ north$\\

$$ & $north\ south$ & $\rightarrow$ & $south\ north$\\
$$ & $north\ west$ & $\rightarrow$ &  $west\ north$\\
$$ & $east\ north$ & $\rightarrow$ &  $north\ east$\\
$$ & $east\ south$ & $\rightarrow$ &  $south\ east$\\
$$ & $east\ west$ & $\rightarrow$ &  $west\ east$\\
$$ & $south\ north$ & $\rightarrow$ &  $north\ south$\\
$$ & $south\ east$ & $\rightarrow$ &  $east\ south$\\
$$ & $south\ west$ & $\rightarrow$ &  $west\ south$\\
$$ & $west\ north$ & $\rightarrow$ &  $north\ west$\\
$$ & $west\ east$ & $\rightarrow$ &  $east\ west$\\
$$ & $west\ south$ & $\rightarrow$ &  $south\ west$\\
\end{tabular}
\caption{Грамматика для движений Манхэттонской черепашки}
\end{figure}

По поводу второго правила заметим, что некоторые авторы требуют, чтобы хотя бы один из символов левой части был нетерминалом. Это ограничение можно легко обойти введением новых нетермналов.\\
	Простой кольцевой маршрут $north\ east\ south\ west$ выводится, как показано на рис. 2.5 (символы сокращены). Обратите внимание пустую альтернативу ($\varepsilon$) первого правила, которая приводит к "`исчезновению"' третьего символа $M$ в графе вывода.\\
\begin{figure}[h]
	\centering
		\includegraphics[width=5cm]{figs/gener/fig25}		
	\caption{Вывод цикличного пути с помощью грамматики 2.4}
\end{figure}

%стр 18
\section{Иерархия Хомского грамматик и языков}
Грамматики с рис. 2.2 и 2.4 легко понять, но некоторые простые грамматики выводят очень сложные множества. Грамматика для любого конретного множества обычно вряд ли является простой. (Мы говорим "`грамматика"', хотя, конечно же, существует множество \textit{грамматик} для множества. Под определенем \textit{грамматика} подразумевается любая грамматика, выполняющая нужную нам работу.) Если множество можно сгенерировать (не важно \textit{как}), то оно может быть сгенерировано и с помощью грамматики. Но мы не можем знать, будет ли это легко сделать и что полученная грамматика будет простой и понятной. Рекомендуется напиать грамматику для нашей черепашки, которая не может улететь на запад относительно начальной точки\footnote{Железный занавес давно рухнул, но не для черепашки - Прим. пер.}. Подсказка: используйте специальный нетерминальный маркер для каждого блока, когда черепашка находится восточнее начальной точки.\\
	Помимо интеллектуальной проблемы, грамматики несут ещё и фундаментальную и практическую проблемы. Мы увидим, что для них не существует универсального алгоритма разбор, а те, что сущесвтуют - либо очень неэффективны, либо очень сложны. См. раздел 3.5.2. \\
	Желание ограничить неуправляемость грамматики и одновременно максимально сохранить её мощь, привело к появлению для грамматик \textit{Иерархии Хомского}. Эта иерархия включает в себя 4 различных типа грамматик, обозначенных от 0 до 3, а также и особый пятый тип, обозначаемый Тип4. Грамматика нулевого типа это (неограниченная) PS грамматика, которую мы только что рассматривали выше. В других типах грамматик возможные формы правил отличаются б$\acute{o}$льшими ограничениями. Каждое из этих ограничений хорошо обоснованно: грамматика становится проще, но также имеет меньшую мощность. К счастью, эти менее мощные типы все ещё очень полезны. Вообще, они полезны даже больше, чем нулевой тип грамматик. Давайте теперь рассмотрим каждый из этих трёх оставшихся типов, а также и о тривиальном, но не менее полезным, четвёртом типе.\\
	
%15.05.06
\subsection{Грамматики первого типа}
	Характерной особенностью грамматики \textit{нулевого} типа является то, что она может иметь правила трансформации любого (ненулевого) количества символов в любое (возможно - нулевое) количество символов. Пример: $$,\ N\ E \rightarrow\ and\ N$$ в котором три символа заменяются двумя. Исключив (отменив) такое поведение получим первый тип грамматик. Удивительно, но существует два совершенно различных определения грамматики Типа 1, которые доказанно эквивалентны.\\
	Грамматика является \textit{монотонной} первого типа, если она не содержит правил, в которых левая часть имеет больше символов, чем правая. Это, например, запрещает правила вида $N\ E\ \rightarrow\ and\ N$.\\
	Грамматика является \textit{контекстно-зависимой} первого типа, если все её правила контекстно-зависимы. Правило является \textit{контекстно-зависимым} если только один (нетерминальный) символ в её левой части заменяется символами, в то время, как другие символы правой части остаются нетронутыми (в том же порядке). Пример:\\
$$ Name\ Comma\ Name\ End\ \rightarrow\ Name\ and\ Name\ End$$
в котором правило $$ Comma\ \rightarrow\ and$$ применяется, если в левый контекст входит $Name$ и в правый контекст входят $Name\ End$. Сам контекст остаётся нетронутым. При замене смениться должен хотя бы один символ. Это означает, что контектно-зависимые грамматики всегда монотонные; см. Раздел 2.6.\\
	Далее приведена монотонная грамматика для нашего примера с Томом, Диком и Гарри. При создании монотонной грамматики не следует производить больше символов, чем нужно. Здесь мы сохраняем маркер окончания предложения под видом самого правого имени.\\
\begin{center}
\begin{figure}[h]
\HSp
\label{figcontsens1}
\begin{tabular}{rcl}
\GrT{$Name$} & $\rightarrow$ & $tom\ |\ dick\ |\ harry$\\
\GrT{$Sentence_S$} & $\rightarrow$ & $Name\ |\ List$\\
\GrT{$List$} & $\rightarrow$ & $EndName\ |\ Name\ ,\ List$\\
\GrT{$,\ EndName$} & $\rightarrow$ & $and\ Name$\\
\end{tabular}
\end{figure}
\end{center}
где $EndName$ это один символ.\\
	А вот контекстно-зависимая грамматика для этого примера:\\
\begin{center}
\begin{figure}[h]
\HSp
\label{figcontsens1}
\begin{tabular}{rcll}
\GrT{$Name$} & $\rightarrow$ & $tom\ |\ dick\ |\ harry$\\
\GrT{$Sentence_S$} & $\rightarrow$ & $Name\ |\ List$\\
\GrT{$List$} & $\rightarrow$ & $EndName\ |\ Name\ |\ Comma\ List$\\
\GrT{$Comma\ EndName$} & $\rightarrow$ & $and\ EndName$ & контекст: $\ldots\ EndName$\\
\GrT{$and\ EndName$} & $\rightarrow$ &   $and\ Name$ & 			контекст: $and\ \ldots$\\
\GrT{$Comma$} & $\rightarrow$ & $,$\\
\end{tabular}
\end{figure}
\end{center}

Обратите внимание, что здесь применяется дополнительный нетерминал $Comma$ чтобы вывести терминал $and$ в нужном контексте.\\
	Монотонные и контекстно-зависимые грамматики одинаковы по мощности: если монотонная грамматика генерирует определённый язык, то и контекстно-зависимая грамматика может его сгенерировать, и наоборот. У них меньше мощности, чем у грамматик нулевого типа (Type 0), то есть существуют языки, которые могут быть сгенерированны грамматикой нулевого типа, но не первого (Type 1). На удивление, мы не имеем простых примеров таких языков. Тем не менее, разница между этими типами грамматик не просто выдумка Мистера Хомского. Грамматика, которая показывает разницу между типами 0 и 1 очень сложна в написании. Но её наличие доказано (см. Хопкрофт и Ульман (Hopcroft and Ulman) [Books 1979, стр. 183-184] или Ривеца (Revesz) [Books 1985, стр. 98]).\\
%16/05/06
	Безусловно, любая грамматика первого типа также является грамматикой нулевого типа, так как первый тип грамматик является подмножеством нулевого (добавлены ограничения). Тем не менее, будет странно называть грамматику первого типа грамматикой нулевого типа. Это равноценно тому, что кошку называть млекопитающим: в принципе, верно, но не так информативно. Грамматику называют по самой большой цифре типа, которой она отвечает.\\
		Мы увидели, что наш пример с Томом, Диком и Гарии сначала был сгенерирован с помощью грамматики нулевого типа, а потом и с помощью грамматики первого типа. Далее мы узнаем, что существует и грамматики второго и третьего, но не четвёртого типов для этого языка. Следовательно, этот язык является языком третьего типа, грамматика для которого является самой простой. Из этого следует, что: Язык типа \textit{n} может быть сгенерирован грамматикой типа \textit{n} или ещё более, но не менее мощной (типа \textit{n+1}); а также, что если язык сгенерирован грамматикой типа \textit{n}, то это не значит, что не существует менее мощной грамматики типа \textit{n+1} для него. Использование нулевой грамматики для нашего примера являлось сильным перебором, но мы вынуждены были сделать это лишь для примера.\\
		Стандартным примером грамматики первого типа является множество слов, содержащих одинаковое количество букв \textbf{a},\textbf{b},\textbf{c} в слудующем порядке:\\
	%рис.
	
\sss{Построение грамматики первого типа}
Сейчас мы будем постепенно создавать грамматику для нашего игрушечного примера. Начнём с самого простого:
$$0.\ S\ \rightarrow\ abc$$
Имея один экземпляр $S$ мы желаем добавить в начало ещё символов $a$; если необходимо "`запомнить"' их количество, то нужно одновременно с добавлением в начало добавлять символы и в конец. Мы не знаем, будет ли это $b$ или $c$. Поэтому используем для этих целей новый нетерминал $Q$. Следующее правило определяет добавление в начало и в конец:
$$1. S\ \rightarrow\ abc\ |\ aSQ$$
Если применить это правило трижды, получим следующую сентенциональную форму:\\
$$aaabcQQ$$
Теперь, чтобы получить из этого $aaabbbccc$, необходимо, чтобы каждый символ $Q$ был заменён на два символа - $b$ и $c$, но мы не можем просто написать $$Q\ \rightarrow\ bc$$, так как это вставит $b$ после первого $c$. Можно добиться нужного эффекта применив правило, позволяющее делать вставки между $b$ и $c$. Теперь можно вставить ещё $bc$:
$$2.\ bQc\ \rightarrow\ bbcc$$
Тем не менее, мы не сможем применить это правило, так как символы $Q$ уже стоят справа от $c$; это можно исправить, разрешив $Q$ перейти влево, относительно $c$:
$$3.\ cQ\ \rightarrow\ Qc$$
Давайте теперь последовательно запишем шаги вывода:\\
\begin{center}
\begin{figure}[h]
\HSp
\label{fighopderiv}
\begin{tabular}{ll}
\GrT{$aaabcQQ$} & (Трижды Правило1)\\
\GrT{$aaabQcQ$} & (Правило3)\\
\GrT{$aaabbccQ$} & (Правило2)\\
\GrT{$aaabbcQc$} & (Правило3)\\
\GrT{$aaabbQcc$} & (Правило3)\\
\GrT{$aaabbbccc$} & (Правило2)\\
\end{tabular}
\end{figure}
\end{center}
Обратите внимание, что этот вывод показывает лишь что грамматика способна производить правильные строки. Читателю остаётся только надеяться, что она не произведёт неправильные строки. Эта грамматика приведена на рис. 2.6.\\
\begin{center}
\begin{figure}[h]
\HSp
\label{fighopderiv}
\begin{tabular}{lcl}
\GrT{$S_S$} & $\rightarrow$ & $abc\ |\ aSQ$\\
\GrT{$bQc$} & $\rightarrow$ & $bbcc$\\
\GrT{$cQ$} & $\rightarrow$ & $Qc$\\
\end{tabular}
\caption{Монотонная грамматика для $a^nb^nc^n$}
\end{figure}
\end{center}
Так как дерево вывода для строки $a^3b^3c^3$ занимает достаточно много места, приведём пример вывода строки $a^2b^2c^2$ на рис. 2.7. Эта грамматика монотонна и, следовательно, первого типа. Можно доказать, что не сущесвтует грамматики второго типа для этого языка.\\
\begin{figure}[h]
	\centering
		\includegraphics[width=7cm]{figs/gener/fig27}		
	\caption{Граф вывода строки $a^2b^2c^2$}
\end{figure}

	Грамматику первого типа часто называют \textit{контекстно-зависимой грамматикой} (CS-грамматикой\footnote{От Contex Sensitive - Прим. пер.}). Это имя используют даже если грамматика является монотонной, так как не существует специального обозначения для монотонных грамматик (хотя можно использовать сокращение MT).\\
	
%17.05.06
\subsection{Грамматики второго типа}
	Грамматики второго типа называются \textit{контестно-свободными} (КС\footnote{CF-от англ. Context-Free - Прим. пер.}).Они имеют прямое отношение к контестно-зависимым грамматикам, так как КС грамматика это по сути контестно-зависимая грамматика с пустыми левым и правым контекстами. В результате, грамматика имеет только те правила, в которых в левой части содержится один нетерминал. Вот пример:\\
\begin{center}
\begin{figure}[h]
\HSp
\label{figcs-sample1}
\begin{tabular}{lrcl}
\GrT{$0.$} & $Name$ & $\rightarrow$ & $tom\ |\ dick |\ harry$\\
\GrT{$1.$} & $Sentence_S$ & $\rightarrow$ & $Name\ |\ List and Name$\\
\GrT{$2.$} & $List$ & $\rightarrow$ & $Name\ ,\ List\ |\ Name$\\
\end{tabular}
\end{figure}
\end{center}
	Так как в левой части находится всего один нетерминал, каждый узел в графе вывода обладает следующим свойством: его вывод \textit{не зависит} от вывода его соседей, т.е. продукция не зависит от контекста. Крестообразные фигуры, которые мы видели на рис. 2.3,2.5,2.7 не могут появиться на графе вывода КС грамматики, а значит этот граф можно смело именовать \textit{деревом вывода}. Пример такого дерева приведён на рис. 2.8.
\begin{figure}[h]
	\centering
		\includegraphics[width=8cm]{figs/gener/fig28}		
	\caption{Дерево вывода для КС грамматики}
\end{figure}
Из единственности нетерминала в левой части также следует возможность собирания всех правых частей определённого нетерминала в одно грамматическое правило (мы это уже проделали в вышеприведённой грамматике). Каждое правило звучит как определение левой части:\\
\begin{itemize}
	\item $Sentence$ это $Name$ или $List$, за которым следует $and$, за которым следует $Name$.
	\item $List$ это $Name$, за которым следует $,$, за которым следует $List$, или это $Name$.
\end{itemize}
В реальном мире многие понятия определяются через другие понятия. КС грамматика является очень кратким вариантом записи таких взаимосвязей. Очень тривиальный пример структуры книги приводится далее на рис. 2.9.
\begin{center}
\begin{figure}[h]
\HSp
\label{fig29}
\begin{tabular}{rcl}
\GrT{$Book_S$} & $\rightarrow$ & $Preface\ ChapterSequence\ Conclusion$\\
\GrT{$Preface$} & $\rightarrow$ & $"PREFACE"\ ParagraphSequence$\\
\GrT{$ChapterSequence$} & $\rightarrow$ & $Chapter\ |\ Chapter\ ChapterSequence$\\
\GrT{$Chapter$} & $\rightarrow$ & $"CHAPTER"\ Number\ ParagraphSequence$\\
\GrT{$ParagraphSequence$} & $\rightarrow$ & $Paragraph\ Paragraph\ ParagraphSequence$\\
\GrT{$Paragraph$} & $\rightarrow$ & $SentenceSequence$\\
\GrT{$SentenceSequence$} & $\rightarrow$ & $\ldots$\\
\GrT{$$} & $\ldots$ & $$\\
\GrT{$Conclusion$} & $\rightarrow$ & $"CONCLUSION"\ ParagraphSequence$\\
\end{tabular}
\caption{Простая (и не полноценная) грамматика структуры книги}
\end{figure}
\end{center}
Конечно, это просто КС грамматика, так что от неё можно ожидать генерации и такой структуры книги:\\
\begin{figure}[h]
\DHSp
\begin{tabular}{l}
PREFACE\\
qwertyuiop\\
CHAPTER V\\
asdfghjkl\\
zxcvbnm,.\\
CHAPER II\\
qazwsxedcrfvtgb\\
yhnujmikolp\\
CONCLUSION\\
All cats say blert when walking through walls\\
\end{tabular}
\end{figure}
В этом пример, тем не менее, сохранена правильная структура. Язык SGML\footnote{Дэвид Бэррон (David Barron), "`Why use SGML?"', \textit{Electronic Publishing, т.2, №1, стр 3-24, Апрель 1989. Краткое введение в SGML (Standart Generalized Markup Language) и сравнение его с другими системами}} использует такой подход при подготовке и выделении текста, чтобы управлять структурой документа.\\
	Более короткий, но и более сложный пример грамматики для движения лифта, который всегда возвращается в исходное состояние (Манхэттонская черепашка без двиежния по горизонтали действует аналогично) приведён далее.\\
\begin{center}
\begin{figure}[h]
\HSp
\label{figcs-elev1}
\begin{tabular}{lcl}
\GrT{$ZeroMotion_S$} & $\rightarrow$ & $up\ ZeroMotion\ down\ ZeroMotion$\\
\GrT{$$} & $|$ & $down\ ZeroMotion\ up\ ZeroMotion$\\
\GrT{$$} & $|$ & $\varepsilon$\\
\end{tabular}
\end{figure}
\end{center}
Здесь лифт может двигаться на любую высоту (совсем как в Манхэттэне).\\	
%18.05.06
	Если мы отбросим некоторые детали, то сможем разработать КС структуру для предложений любого человеческого языка, например, английского:\\
\begin{center}
\begin{figure}[h]
\HSp
\label{figcs-english1}
\begin{tabular}{rcl}
\GrT{$Sentence_S$} & $\rightarrow$ & $Subject\ Verb\ Object$\\
\GrT{$Subject$} & $\rightarrow$ & $NounPhrase$\\
\GrT{$Object$} & $\rightarrow$ & $NounPhrase$\\
\GrT{$NounPhrase$} & $\rightarrow$ & $the\ QualifiedNoun$\\
\GrT{$QualifiedNoun$} & $\rightarrow$ & $Noun\ |\ Adjective\ QualifiedNoun$\\\
\GrT{$Noun$} & $\rightarrow$ & $castle\ |\ caterpillar\ |\ cats$\\
\GrT{$Adjective$} & $\rightarrow$ & $well-read\ |\ white\ |\ wistful\ |\ \ldots$\\
\GrT{$Verb$} & $\rightarrow$ & $admires\ |\ bark\ |\ criticize\ |\ \ldots$\\
\end{tabular}
\end{figure}
\end{center}
Такая грамматика может сгенерировать, например, следующее предложение:\\
\begin{center}
the well-read cats criticize the wistful caterpillar
\end{center}
Так как не существует связи с определённым контекстом, грамматика может генерировать совершенно неправильные предложения:\\
\begin{center}
the cats admires the white well-read castle
\end{center}
	Чтобы работать с контекстом, можно создать PS грамматику\footnote{Для PS, в отличие от CF не существует хорошо прижившегося перевода, так что позволю себе далее не переводить PS - Прим. пер.} (для языка попроще):
\begin{center}
\begin{figure}[h]
\HSp
\label{figcs-english1}
\begin{tabular}{rcl}
\GrT{$Sentence_S$} & $\rightarrow$ & $Noun\ Number\ Verb$\\
\GrT{$Number$} & $\rightarrow$ & $Singular\ |\ Plural$\\
\GrT{$Noun\ Singular$} & $\rightarrow$ & $castle\ Singular\ |\ caterpillar\ Singular\ |\ \ldots$\\
\GrT{$Singular\ Verb$} & $\rightarrow$ & $Singular\ admires\ |\ \ldots$\\
\GrT{$Singular$} & $\rightarrow$ & $\varepsilon$\\\
\GrT{$Noun\ Plural$} & $\rightarrow$ & $cats\ Plural\ |\ \ldots$\\
\GrT{$Plural\ Verb$} & $\rightarrow$ & $Plural bark\ |\ Plural\ Criticize\ |\ \ldots$\\
\GrT{$Plural$} & $\rightarrow$ & $\varepsilon$\\
\end{tabular}
\end{figure}
\end{center}
где маркеры $Singular$ и $Plural$ выводят английские слова(существительные в единственном и множественном числе). Тем не менее, эта грамматика позволяет котам лаять (allow cats to bark). См. раздел про грамматики Вингардена, которые лучше взаимодействуют с контекстом (2.4.1).\\
	Множество примеров КС грамматик появились благодаря языкам программирования. Предложения в таких языках (программы) должны быть автоматически обработаны (компилятором). Очень быстро стало понятно (примерно в 1958 году), что это легче сделать с помощью хорошо продуманной грамматики. Синтаксис большинства языков программирования, которые используются сегодня, основан на грамматике\footnote{COBOL и FORTARAN тоже имели грамматику, но она была лишь описательной, а не порождающей}.\\
%E-свободная?
	Некоторые авторы (например, Хомский), как и некоторые алгоритмы разбора, требуют, чтобы КС грамматика была монотонной. Единственным способом получить немонотонную КС грамматику является использование правила с пустой правой частью; такое правило называется $\varepsilon$-правилом, а несодержащая их грамматика называется $\varepsilon$-свободной. Вообще, требование отсутствия $\varepsilon$-правил не является сильным ограничением. Любую КС грамматику можно сделать $\varepsilon$-свободной путём подстановки $\varepsilon$-правил (см. раздел 4.2.3.1), но такой процесс вовсе не улучшает структуру грамматики. Эта неприятность подробнее рассматривается в разделе 2.6.\\

\sss{БНФ}
Существует множество различных форм записи КС грамматик для языков программирования; все они эквивалентны по функциональности. Давайте покажем две из них. Первой будет БНФ (BNF\footnote{Backus-Naur Form}), которая впервые была использована для описания ALGOL 60. Вот пример:\\
\begin{center}
\begin{figure}[h]
\HSp
\label{figcs-bnf1}
\begin{tabular}{ll}
\GrT{$<name>::=$} & $tom\ |\ dick\ |\ harry$\\
\GrT{$<sentence>_S::=$} & $<name>\ |\ <list>\ and\ <name>$\\
\GrT{$<list>::=$} & $<name>,\ <list>\ |\ <name>$\\
\end{tabular}
\end{figure}
\end{center}
Главными отличиями этой нотации являются угловые скобки, в которые заключены нетерминалы, и специальные символы $::=$ ("`может выводить"'). В некоторых случаях правила оканчиваются точкой с запятой.

\sss{Форма ван Вингардена}
Второй основной формой записи КС грамматики является "`грамматка ван Вингардена"'\footnote{van Wijngaarden - Прим. пер.}. Вот пример:\\
\begin{center}
\begin{figure}[h]
\HSp
\label{figcs-vW1}
\begin{tabular}{ll}
\GrT{$name:$} & $tom\ symbol;\ dick\ symbol;\ harry\ symbol.$\\
\GrT{$sentence_S:$} & $name;\ list,\ and\ name.$\\
\GrT{$list:$} & $name,\ comma symbol,\ list;\ name.$\\
\end{tabular}
\end{figure}
\end{center}
Терминальные символы оканчиваются на $\ldots symbol$, их отображение машинно-зависимо и не определено в грамматике. Правила оканчиваются точкой. Знаки пунктуации используются согласно тому, как мы и привыкли. Вот примеры:\\
\begin{center}
\begin{figure}[h]
\HSp
\label{figcs-punct1}
\begin{tabular}{ll}
\GrT{$:$} & - "`определено, как a(n)"'\\
\GrT{$;$} & - "`, или как a(n)"'\\
\GrT{$,$} & - "`,за которым следует а(n)"'\\
\GrT{$.$} & - "`,и ничего более"'\\
\end{tabular}
\end{figure}
\end{center}

Второе правило можно прочитать следующим образом: "`предложение определено, как имя (name), или как список (list), за которым следует символ-и (and), за которым следует имя (name), и ничего более"'. Такая форма записи максимально эффективно используется только когда применяется для двухуровневых грамматик ван Вингардена. Она имеет как определённые достоинства, так и недостатки.\\

\sss{Расширенная КС грамматика}
Можно заметно сократить объём КС грамматики с помощью добавления специальных сокращений для часто используемых конструкций. Давайте вспомним грамматику для книги из рисунка 2.9. Можно встретить там правила следующего вида: $$SomethingSequence\ \rightarrow\ Something\ |\ Something\ SomethingSequence$$ 
В \textit{расширенной КС грамматике} (РКС грамматике\footnote{ECF - Прим. пер.}) мы можем записать такую продукцию просто - $Something^+$, что означает "`один или более символов $Something$"'. Обратите внимание, что теперь не нужно вводить специальное правило для этого символа, так как конструкция $Something^+\ \rightarrow\ Something\ |\ Something\ Something^+$ содержится в грамматике неявно. Соответсвенно этому, существует ещё несколько сокращений: $Something^*$ означает "`нуль или несколько символов $Something$"', а $Something^?$ означает "`нуль или один символ $Something$"'. В этих примерах операторы $^+$,$^*$ и $^?$ относятся к предыдущим символам. Их действие можно расширить на несколько символов путём объеденения в скобки: $(Something\ ;)^?$ означает "`нуль или один (символ $Something$, за которым следует символ-;)"'. Теперь можно записать грамматику для книги следующим образом:\\
\begin{center}
\begin{figure}[h]
\HSp
\label{fig210}
\begin{tabular}{rcl}
\GrT{$Book_S$} & $\rightarrow$ & $Preface\ Chapter^+\ Conclusion$\\
\GrT{$Preface$} & $\rightarrow$ & $"PREFACE"\ Paragraph^+$\\
\GrT{$Chapter$} & $\rightarrow$ & $"CHAPTER"\ Number\ Paragraph+$\\
\GrT{$Paragraph$} & $\rightarrow$ & $Sentence+$\\
\GrT{$Sentence$} & $\rightarrow$ & $\ldots$\\
\GrT{$$} & $\ldots$ & $$\\
\GrT{$Conclusion$} & $\rightarrow$ & $"CONCLUSION"\ Paragraph^+$\\
\end{tabular}
\caption{РКС грамматика для структуры книги}
\end{figure}
\end{center}
Расширение грамматики отнюдь не увеличиваtт её мощность: любые неявные продукции могут быть записаны явно. Преимущество такой грамматики заключается в её простоте. Обозначение со звездочкой (например, $X^*$, что означает "`последовательность из нуля или более X"') называется \textit{замыканием Клини}. Если $X$ является множеством, то запись $X^*$ нужно читать как "`последовательность из нуля или более элементов множества X"'. Вспомните звёздочку в $\Sigma^*$ из раздела 2.1.3.3. Запись с использованием операторов $^*$,$^+$ и $^?$ называют \textit{регулярным выражением}. РКС, которые имеют регулярные выражения в правых частях, иногда называют \textit{Regular Right Part grammar}(или просто RRP-грамматикой\footnote{Для RRP, как и для CS и PS не существует хорошо прижившегося перевода, так что позволю себе не переводить RRP - Прим. пер.}). Такое название лучше отражает суть грамматики, но его труднее произносить.\\
	Существует две "`школы"', каждая из которых по-своему интерпретирует регулярную правую часть. Одна школа говорит, что правило вида $Book\ \rightarrow\ Preface\ Chapter^+\ Conclusion$ это сокращение для 
\begin{center}
\begin{figure}[h]
\HSp
\label{fig-rightder1}
\begin{tabular}{rcl}
\GrT{$Book$} & $\rightarrow$ & $Preface\ \alpha\ Conclusion$\\
\GrT{$\alpha$} & $\rightarrow$ & $Chapter\ |\ Chapter\ \alpha$\\
\end{tabular}
\end{figure}
\end{center}
% буз дефиса?
Это называется право-рекурсивной формой. Её преимущество заключается в том, что её просто представить и очень просто перевести в "`обычную"' КС запись. Недостатком является то, что такие преобразования влекут за собой появление ананимных правил (обозначенных здесь как $\alpha$) и то, что получившееся дерево вывода очень кособокое. Книга, состоящая из четырёх глав не соответсвует нашему представлению.
\begin{figure}[h]
	\centering
		\includegraphics[width=7cm]{figs/gener/fig211}		
	\caption{Дерево вывода для праворекурсивной реализации}
\end{figure}

	Вторая школа считает, что $Book\ \rightarrow\ Preface\ Chapter^+\ Conclusion$ это сокращение для
\begin{center}
\begin{figure}[h]
\HSp
\label{fig-rightder2}
\begin{tabular}{rcl}
\GrT{$Book$} & $\rightarrow$ & $Preface\ Chapter\ Conclusion$\\
\GrT{$$} & $|$ & $Preface\ Chapter\ Chapter\ Conclusion$\\
\GrT{$$} & $|$ & $Preface\ Chapter\ Chapter\ Chapter\ Conclusion$\\
\GrT{$$} & $|$ & $\ldots$\\
\GrT{$$} & $\ldots$ & $$\\
\end{tabular}
\end{figure}
\end{center}
Это так называемая "`итеративная"' интерпретация. Плюсом является то, что получаемое дерево вывода (рис. 2.12) имеет хороший вид, но её минус заключен в том, что она требует бесконечного числа правил вывода, а также из узлов дерева может выходить различное число ветвей.
\begin{figure}[h]
	\centering
		\includegraphics[width=11cm]{figs/gener/fig212}		
	\caption{Дерево вывода для итеративной реализации}
\end{figure}
	Так как реализация итеративной версии далека от тривиальной, большинство разборщиков использует рекурсивную форму в различных её ипостасях. Тем не менее, большинство исследований проводится с итеративной версией реализации.\\
	
%19.05.06
\subsection{Грамматики третьего типа}
Основным свойством КС грамматики является то, что они описывают вещи, которые включают другие вещи: определённый обект может содержать другие объекты в различных местах, которые, в свою очередь, могут содержать ... и т.д. Когда в процессе вывода мы выводим один из объектов, правая часть всё ещё "`помнит"', что должно идти после него. Например, в грамматике для английского языка правая часть $Subject\ Verb\ Object$, после вывода нетерминала для $Subject$, всё ещё "`помнит"', что нужно далее вывести $Verb$. В то время, как мы работаем с $Subject$, символы $Verb$ и $Object$ остаются на очереди в правой части сентенциальной формы. Например, 
$$the\ wistful\ QualifiedNoun\ Verb\ Object$$
В правой части $$up\ ZeroMotion\ down\ ZeroMotion$$ после обработки $up$ и произвольно сложного $ZeroMotion$, правая часть указывает, что далее должен следовать $down$.\\
	Приведение к третьему типу запрещает это: правая часть должна иметь только один нетерминал и он должен быть последним в ней. Это означает, что доступны для использования лишь два типа правил\footnote{Сущесвует промежуточный тип грамматик (2.5), в которых нетерминал не обязан находиться в последней позиции. Такие грамматики называются линейными (linear)}:\\
\begin{itemize}
	\item Нетерминал, производящий нуль или более терминалов
	\item Нетерминал, производящий нуль или более терминалов, после которого следует единственный нетерминал
\end{itemize}
Вообще, грамматика третьего типа по Хомскому ограничивается следующими видами правил:
\begin{itemize}
	\item Нетерминал, производящий один терминал
	\item Нетерминал, производящий один терминал, после которого следует один нетерминал
\end{itemize}
Наше определение эквивалентно, так как конвертация грамматики к типу 3 по Хомскому является не таким уж тривиальным процессом.\\
	Третий тип грамматик также называют \textit{регулярными грамматиками}(RE) или \textit{грамматиками конечного автомата}(FS\footnote{От англ. Finit-state - Прим. пер}). Так как регулярные грамматики довольно часто используются для описания текста на уровне символов, терминальные символы обычно состоят из одной буквы. Можно сократить $Tom$ до $t$,$Dick$ до $d$, $Harry$ до $h$ и заменить $and$ на  $\verb|&|$. На следующем рисунке приведена грамматика третьего типа:\\
\begin{center}
\begin{figure}[h]
\HSp
\label{fig-3type1}
\begin{tabular}{rcl}
\GrT{$Sentence_S$} & $\rightarrow$ & $t\ |\ d\ |\ h\ |\ List$\\
\GrT{$List$} & $\rightarrow$ & $t\ ListTail\ |\ d\ ListTail\ |\ h\ ListTail$\\
\GrT{$ListTail$} & $\rightarrow$ & $,\ List\ |\ \verb|&|t\ |\ \verb|&|d\ |\ \verb|&|h$\\
\end{tabular}
\caption{Пример грамматики 3 типа}
\end{figure}
\end{center}
	Дерево вывода предложения для грамматики третьего типа представляет собой последовательность терминалов, к которым слева примыкают нетерминалы. На рис. 2.14 приведён пример.\\
%рис. 2.14 (граф)
\begin{figure}[h]
	\centering
		\includegraphics[width=7cm]{figs/gener/fig214}		
	\caption{Дерево вывода для регулярной грамматики (3 тип)}
\end{figure}

	Повторяющаяся структура, как в вышеприведённом примере, типична для регулярных грамматик. Чтобы избежать этого, было разработано множество нотаций. Самая распространённая из них использует квадратные скобки, которые означают "`любй из множества символов"', для сокращения. $t\ |\ d\ |\ h$ можно записать так - $[tdh]$:
\begin{center}
\begin{figure}[h]
\HSp
\label{fig-3type2}
\begin{tabular}{rcl}
\GrT{$S_S$} & $\rightarrow$ & $[tdh]\ |\ L$\\
\GrT{$L$} & $\rightarrow$ & $[tdh]\ T$\\
\GrT{$T$} & $\rightarrow$ & $,\ L\ |\ \verb|&|\ [tdh]$\\
\end{tabular}
\end{figure}
\end{center}
Первое время такая запись вызывает негативные эмоции, но чуть позже вы ощутите её пользу. Грамматику можно упростить даже ещё больше:
\begin{center}
\begin{figure}[h]
\HSp
\label{fig-3type3}
\begin{tabular}{rcl}
\GrT{$S_S$} & $\rightarrow$ & $[tdh]\ |\ L$\\
\GrT{$L$} & $\rightarrow$ & $[tdh]\ ,\ L\ |\ [tdh]\ \verb|&|\ [tdh]$\\
\end{tabular}
\end{figure}
\end{center}
%20.05.06
	Другим вариантом является использование макросов, т.е. специальных имён для частей грамматики, которые подставляются до первого использования:
\begin{center}
\begin{figure}[h]
\HSp
\label{fig-3type4}
\begin{tabular}{rcl}
\GrT{$Name$} & $\rightarrow$ & $t\ |\ d\ |\ h$\\
\GrT{$S_S$} & $\rightarrow$ &   $\verb|$|Name\ |\ L$\\
\GrT{$L$} & $\rightarrow$ &   $\verb|$|Name\ ,\ L\ |\ \verb|$|Name\ \verb|&|\ \verb|$|Name$\\
\end{tabular}
\end{figure}
\end{center}
Популярный генератор регулярных грамматик \textit{lex} (написанный Леском и Шмидом (Lesk and Schmidt [FS 1975]) позволяет использовать оба варианта.\\
	Обратите внимание, что если мы будем придерживаться стандартной формы грамматики третьего типа Хомского, то она не сможет быть менее объемной, чем приведённая далее:
%?
\begin{center}
\begin{figure}[h]
\HSp
\label{fig-3type5}
\begin{tabular}{rcl}
\GrT{$S_S$} & $\rightarrow$ &   $t\ |\ d\ |\ h\ |\ tM\ |\ dM\ |\ hM$\\
\GrT{$M$} & $\rightarrow$ &   $,N\ |\ \verb|&|P$\\
\GrT{$N$} & $\rightarrow$ &   $tM\ |\ dM\ |\ hM$\\
\GrT{$P$} & $\rightarrow$ &   $t\ |\ d\ |\ h$\\
\end{tabular}
\end{figure}
\end{center}

Такая форма грамматики заметно проще для обработки, но не так удобна для нас с вами, как, например, \textit{lex}. Можно заметить, что формальный-лингвист использует ничего не значащие имена, в то время как инженер обычно жертвует скоростью обработки, но предпочитает чёткое обозначение концепций, описываемых грамматикой (например, $\verb|$|Name$...).\\
	Давайте подведём итоги Во-первых, сентенциальные формы регулярных грамматик имеют лишь один нетерминал, который находится в их конце. Вот пример:
%?
\begin{center}
\begin{figure}[h]
\HSp
\label{fig-3type6}
\begin{tabular}{l}
$Sentence_S$\\
$List$\\
$t\ ListTail$\\
$t\ ,\ List$\\
$t\ ,\ d\ ListTail$
$t\ ,\ d\ \verb|&|\ h$
\end{tabular}
\end{figure}
\end{center}

	Во-вторых, любая регулярная грамматика может быть заметно сокращена при записи с помощью операторов $^*$,$^+$ и $^?$, о которых речь шла в разделе 2.3.2. Используя эти операторы вместе со скобками, можно описать нашу грамматику следующим образом:
$$S_S\ \rightarrow\ ((\ [tdh],\ )^*\ [tdh]\verb|&|)^?\ [tdh]$$
В этом примере скобки служат для разграничения операторов. Регулярные выражения могут быть использованы для любой грамматики третьего типа. Не забудьте, что операторы относятся к тому, что идёт слева от них. Чтобы отличать их от обычных операторов умножения и сложения, обычно при их написании используют приподнятый шрифт, но при вводе в компьютер они записываются аналогично обычным операторам.\\

\subsection{Грамматики четвёртого типа}
	Сейчас мы должны наложить самое последнее ограничение на возможности грамматики, а именно: запретить нетерминалы в правой части. Это сильно ограничивает производящюю мощь, по сути оставляя нам только возможность выбора альтернатив. Начальный символ имеет (конечный) список альтернатив, из которых мы имеем право выбирать. Это отражено в названии таких грамматик - грамматики конечного выбора (КВ-грамматика\footnote{FC - от англ. Finite-Choice - Прим. пер.}).\\
	Для нашего $t,d\verb|&|h$ языка не существует КВ-грамматики. Тем не менее, если мы уверенны, что предложение будет состоять менее чем из конечного $n$ (например, 100) имён, то можно построить грамматику, которая просто будет перечислять все возможные комбинации. Для $n=3$ получим следующую грамматику:
	$$S_S\ \rightarrow\ [tdh]\ |\ [tdh]\ \verb|&|\ [tdh]\ |\ [tdh]\ ,\ [tdh]\  \verb|&|\ [tdh]$$
в которой будет $3+3*3+3*3*3=39$ правил вывода.\\
% reasoning?
	КВ-грамматики не представлены в иерархии Хомского, то есть они не определены Хомским. Тем не менее, они часто используются при анализе. Множество зарезервированных слов языка программирования может быть описано КВ-грамматикой. Многие грамматики являются КВ лишь частично, т.е. лишь некоторые продукции являются КВ. Например, первое правило на рис. 2.2 было КВ. Другим примером КВ-правила являлся макрос из раздела 2.3.3. Нам не понадобятся макросы, если мы изменим в определении регулярной грамматики "`один или более нетерминалов"' на "`один или более нетерминалов, либо КВ-нетерминалы"'. В конце концов КВ нетерминалы могут вывести только конечное число терминалов.\\
	
%21.05.06
\section{VW-грамматики}	
% как озаглавить?
\subsection{Недостатки CS и PS грамматик}
Чуть выше мы рассмотрели иерархию грамматик следующего вида:
\begin{itemize}
	\item PS-грамматики
	\item CS-грамматики (контекстно-зависимые)
	\item CF-грамматики (контекстно-свободные или КС)
	\item RE-грамматики (регулярные)
	\item FC-грамматики (конечного выбора или КВ)
\end{itemize}

Безусловно, границы каждого вида грамматик чётко очерчены, но некоторые грамматики сильнее выделяются на фоне других. Два вида грамматик имеют серьёзные отличия от других двух: контекстно-зависимые грамматики противопоставлены контекстно-свободным грамматикам (КС), а регулярные - грамматикам конечного выбора (КВ). Разница последних заключается лишь в том, что одна из них продуктивна, а вторая - нет.\\
	Контекстно-зависимые грамматики "`глобально-зависимы"', а КС грамматики "`локально-независимы"'. Как только в КС выведен нетерминал, его последующее построение не зависит от оставшейся части сентенциальной формы. В контекстно-зависимой грамматике нетерминал должен "`оглядеться"' на своих соседей, чтобы установить, какие продукции доступны для него в данный момент. Локальная независимость КС грамматик означает, что зависимости не могут быть описаны ими (кроме "`местных"' взаимодействий). Тем не менее, такие зависимости часто очень интересно изучать, так как они прекрасно описывают многие свойства текста, и могли бы применяться, например, при использовании переменных в программе или для описания "`рекуррентной"' структуры музыкальной композиции. Если мы описываем такие языки с помощью КС грамматики, то не можем использовать корреляции между символами, так что часто приходится использовать "`внешнюю"' проверку: сначала разбираем текст, а потом проверяем соответсвующие связи внешней программой. Этого хотелось бы избежать, так как это сводит плюсы грамматики к нулю, главным из которых является максимально полноценное описание всех свойств текста.\\
	Очевидным решением такой проблемы является использование контекстно-зависимой грамматики для описания связей (что и является контекстной-зависимостью), но тогда мы получим уже проблему практического характера: контекстно-зависимая грамматика \textit{может} описать связи, но \textit{не таким образом}, как их понимает человек. Очень полезно было бы сравнить КС грамматику из раздела 2.3.2 с контекстно-зависимой грамматикой (которая как раз и описывает взаимосвязи) для языка $a^nb^nc^n$, приведённой на рис. 2.6. Грамматику для структуры книги (рис. 2.9) легко понять, а вот грамматику с рис. 2.6 трудно понять, даже если мы помним, как она была построена. Дело не в коротких называниях символов (например, $Q$), ведь и версия с длинными (и информативными) именами (рис. 2.15) не менее загадочна. Можно ожидать, что описав грамматику для $a^nb^nc^n$, очень просто описать и грамматику для $a^nb^nc^nd^n$. Но это не так. Грамматика для $a^nb^nc^nd^n$ намного сложнее (в том числе и для понимания) и требует пересмотра задачи. 
	
\begin{center}
\begin{figure}[h]
\HSp
\label{figmonoton3}
\begin{tabular}{rcl}
\GrT{$S_S$} & $\rightarrow$ &   $a\ b\ с\ |\ a\ S\ bcpack$\\
\GrT{$b\ bcpack\ c$} & $\rightarrow$ &   $b\ b\ с\ c$\\
\GrT{$c\ bcpack$} & $\rightarrow$ &   $bcpack\ c$\\
\end{tabular}
\caption{Монотонная грамматика для $a^nb^nc^n$ с информативными именами}
\end{figure}
\end{center}
	Причиной этих недостатков является то, что контекстно-зависимые и PS грамматики обладают такими возможностями только как продолжениями локальных зависимостей. Теоретически, изучение окружения (т.е. то, что мы подразумевали под фразой "`оглядеться на своих соседей"') уже достаточно для описания любых глобальных связей, но такой подход заставляет данные преодолевать огромные дистанции по сентенциональным формам. Например, при выводе $a^4b^4c^4$ можно заметить, что многие $bcpack$ вынуждены много перемещаться. В любой контекстно-зависмой грамматике множество "`посланцев"' перемещается вниз и вверх, чтобы донести информацию о выводе из далёких краёв. Хотя это интересно представить, это заставляет большинство правил иметь какую-то информацию о других правилах. Это и делает грамматику сложной.\\
		Сущесвует несколько попыток исправить эту ситуацию, например: индексированные грамматики (Ахо) (indexed grammar (Aho [1968])), запоминающие грамматики (Барт) (recording grammar (Barth [PSCS 1979])), аффиКС грамматики (Костер) (affix grammar (Koster [VW 1971])), VW-грамматики (van Wijngaarden [VW 1969]). Последние и являются самыми эффективными и наглядными, так что следует рассмотреть именно их. Аффиксные грамматики подробно обсуждаются в разделе 2.4.5.\\
		
\subsection{VW-грамматики}
Вообще, не верно считать, что КС грамматики не могут описать глобальные связи. Дело в том, что они могут описать лишь конечное их множество. Пусть мы имеем язык, который содержит символы $begin$, $middle$ и $end$. Пусть имеется три варианта $begin$ и $end$. На рис. 2.16 подразумевается, что каждый $end$ будет соответсвовать своему $begin$. 
\begin{center}
\begin{figure}[h]
\HSp
\label{figlr-cf1}
\begin{tabular}{rcl}
\GrT{$text_S$} & $\rightarrow$ &   $begin1\ middle\ end1$\\
\GrT{$$} & $|$ &   $begin2\ middle\ end2$\\
\GrT{$$} & $|$ &   $begin3\ middle\ end3$\\
\end{tabular}
\caption{Глобальные связи в КС грамматике}
\end{figure}
\end{center}
%23.05.06
Можно представить, что для $begin1$ и $end1$ используются круглые скобки ( и ), для $begin2$ и $end2$ используются квадратные скобки [ и ], а для $begin3$ и $end3$ фигурные скобки { и }. КС грамматика сама будет "`подбирать"' закрывающую скобку.\\
	Расширяя КС грамматику мы можем описать большее число глобальных связей. Если мы создадим её бесконечно большой, то, наверное, сможем описать полноценный аналог контектно-независимой грамматики. Вот мы и подошли к основной идее VW-грамматик. Правила бесконечной КС грамматики  оббразуют конечный набор строк, т.е. язык, который и описывается грамматикой. Это называется "`двухуровневой грамматикой"'.\\
%двУуровневая?
	Чтобы было проще описать концепцию такой грамматики и технику её создания, рассмотрим небольшой пример построения VW-грамматики для вышеприведённого языка $L\ =\ a^nb^nc^n$ для любого $n\geq 1$. Будем использовать VW-обозначения, которые рассматриваются в 2.3.2.2: имена терминалов оканчиваются на $symbol$, альтернативы отделяются точкой с запятой (;), а члены альтернатив отделяются запятой (,), что позволяет нам использовать пробел в именах нетерминалов. Вместо стрелки используется двоеточие (:).\\
	Мы можем описать язык $L$ через КС грамматику, если есть возможность использовать бесконечно-большую грамматику:
\begin{center}
\begin{figure}[h]
\HSp
\label{figlr-cfcs}
\begin{tabular}{rcl}
\GrT{$text_S:$} & $a\ symbol,\ b\ symbol,\ c\ symbol;$\\
\GrT{$$} & $a\ symbol,\ a\ symbol,\ b\ symbol,\ b\ symbol,\ c\ symbol,\ c\ symbol;$\\
\GrT{$$} & $a\ symbol,\ a\ symbol,\ a\ symbol,\ b\ symbol,\ b\ symbol,\ c\ symbol,\ c\ symbol,\ c\ symbol;$\\
\GrT{$$} & $\ldots\ \ldots$\\
\end{tabular}
\end{figure}
\end{center}
	Теперь нам предстоит разобраться с бесконечностью с помощью создания грамматики, которая позволит производить вывод аналогично выгеприведённой. Для начала неплохо было бы образовать бесконечный список имён нетерминалов:
\begin{center}
\begin{figure}[h]
\HSp
\label{figlr-cfcs2}
\begin{tabular}{ll}
\GrT{$text_S:$} & $ai,\ bi,\ ci;$\\
\GrT{$$} & $aai,\ bii,\ cii;$\\
\GrT{$$} & $aiii,\ biii,\ ciii;$\\
\GrT{$$} & $\ldots\ \ldots$\\
\end{tabular}
\end{figure}
\end{center}
Теперь создадим три бесконечные группы правил для наших нетерминалов:
\begin{center}
\begin{figure}[h]
\HSp
\label{figlr-cfcs3}
\begin{tabular}{ll}
\GrT{$ai:$} & $a\ symbol.$\\
\GrT{$aii:$} & $a\ symbol,\ ai.$\\
\GrT{$aiii:$} & $a\ symbol,\ aii.$\\
\GrT{$\ldots$} & $ \ldots$\\

\GrT{$bi:$} & $b\ symbol.$\\
\GrT{$bii:$} & $b\ symbol,\ bi.$\\
\GrT{$biii:$} & $b\ symbol,\ bii.$\\
\GrT{$\ldots$} & $ \ldots$\\

\GrT{$ci:$} & $c\ symbol.$\\
\GrT{$cii:$} & $c\ symbol,\ ci.$\\
\GrT{$ciii:$} & $c\ symbol,\ cii.$\\
\GrT{$\ldots$} & $ \ldots$\\
\end{tabular}
\end{figure}
\end{center}

Здесь $i$ означают номера символов $a$, $b$ и $c$. Теперь позволим себе ввести новый вид правил - \textit{метаправило}. Вместо вывода (части) предложения, такие правила будут выводить (часть) имён грамматики. В нашем случае, нужно создать метаправило $N$, которое будет "`выводить"' повторяющиеся $i$:
$$N\ ::\ i\ ;\ i\ N\ .$$
Обратите внимание, что обозначение метаправила чуть-чуть отличается от обычного: левая и правая часи отделены двумя двоеточиями (::), а члены отделяются пробелами ( ). Метаправило $N$ "`выводит"' $i,ii,iii, \ldots$, которые и являются именно теми часятми имён, что нам нужны.\\
	Применим метаправила для "`сворачивания"' четырёх бесконечных групп к четырём \textit{конечным} шаблонным правилам, названных \textit{гиперправилами}.
\begin{center}
\begin{figure}[h]
\HSp
\label{figlr-cfcs4}
\begin{tabular}{ll}
\GrT{$text_S:$} & $a\ N,\ b\ N,\ c\ N$\\

\GrT{$a\ i:$} & $a\ symbol.$\\
\GrT{$a\ i\ N:$} & $a\ symbol,\ a\ N.$\\

\GrT{$b\ i:$} & $b\ symbol.$\\
\GrT{$b\ i\ N:$} & $b\ symbol,\ b\ N.$\\

\GrT{$c\ i:$} & $c\ symbol.$\\
\GrT{$c\ i\ N:$} & $c\ symbol,\ c\ N.$\\
\end{tabular}
\end{figure}
\end{center}
%p. 34

%27.05.06
Каждое исходное правило может быть получено из гиперправила путём подстановки продукций $N$ в каждое вхождение $N$ этого гиперправила. Все одинаковые продукции $N$ должны быть произведены последовательно. Чтобы отличить эти полуоконченные комбинации прописных букв и метаобозначений (например, $a\ N$ или $b\ i\ N$) от обыкновенных имён, их называют \textit{гиперобозначениями} (hypernotion). Подстановка, к примеру, $N=iii$ в гиперправло 
$$b\ i\ N:\ b\ symbol,\ b\ N.$$
даёт КС правлио для КС нетерминала $biiii$
$$biiii:\ b\ symbol,\ biii.$$
	Мы можем использовать такой подход для группировки конечных частей грамматики. Создадим метаправило $A$ для символов $a,b,c$. Опять, нельзя забывать, что все метаобозначения $A$ должны быь заменены последовательно. Результат показан на рис. 2.17.\\
\begin{center}
\begin{figure}[h]
\HSp
\label{figlr-cfcs5}
\begin{tabular}{ll}
\GrT{$N\ ::$} & $i\ ;\ i\ N\ .$\\
\GrT{$A\ ::$} & $a\ ;\ b\ ;\ c\ .$\\
\GrT{} & $$ \\

\GrT{$text_S:$} & $a\ N,\ b\ N,\ c\ N$\\
\GrT{$A\ i:$} & $A\ symbol$\\
\GrT{$A\ i\ N:$} & $A\ symbol,\ A\ N$\\
\end{tabular}
\caption{VW-грамматика для языка $a^nb^nc^n$}
\end{figure}
\end{center}
	Такая грамматика хорошо отображает язык: как только выбрано "`значение"' метаобозначения $N$, следует однозначный вывод. Теперь очень легко расширить грамматику до грамматики для языка $a^nb^nc^nd^n$. Глобальные связи теперь образуются без так называемых символов-"`посланцев"': связи образуются \textit{до} того, как становятся глобальными с помощью последовательной подстановки метаобозначений в простых правых частях. "`Правило последовательной подстановки"' метаобозначений является основой двухуровнего механизма, без этого VW превращается в простую КС (Meersman and Ronzenberg [VW 1978]).\\
	Очень хорошее, также полноценное описание VW-грамматик было написано Крейгом Кливлендом и Узгалисом (Craig Cleaveland and Uzgalis [VW 1977]). Синтцофф (Sintzoff [VW 19676]) доказал, что VW-грамматики имеют такую же мощность, как и PS грамматики, что также показало, что добавление третей ступени не может увеличить мощность всей конструкции. Ван Вингарден (Van Wijngaarden [VW 1974]) показал, что метаграмматика обязана быть лишь регулярной.\\
	
%28/05/06
\subsection{Бесконечные наборы символов}
	Вообще, VW-грамматики сильнее PS: если имя символа может быть сгенерированно грамматикой, то с её помощью можно описать бесконечный набор символов. Безусловно, это всего лишь скрывает реальную проблему: отображение символов на их имена должно быть конечным. VW-грамматика с рис. 2.18 генерирует предложения, состоящие из произвольного числа блоков, каждый из который содержит одинаковое число одинаковых символов. Приведём для примера такие предложение: $s_1s_1s_1s_2s_2s_2$ и $s_1s_1s_2s_2s_3s_3s_4s_4s_5s_5$, где $s_n$ представляет собой $i^n$ сиволов. Минимальная длина блока ограничена 2, чтобы исключить возможность вывода $\sum ^*$.\\
\begin{center}
\begin{figure}[h]
\HSp
\label{figlr-cfcs6}
\begin{tabular}{ll}
\GrT{$N\ ::$} & $n\ N\ ;\ \varepsilon\ .$\\
\GrT{$C\ ::$} & $i\ ;\ i\ C\ .$\\
\GrT{} & $$ \\

\GrT{$text_S:$} & $N\ i\ tail.$\\
\GrT{$N\ C\ tail$} & $\varepsilon ;\ N\ C,\ N\ C\ i\ tail$\\
\GrT{$N\ n\ C\ :$} & $C\ symbol,\ N\ C.$\\
\GrT{$C\ :$} & $\varepsilon .$\\
\end{tabular}
\caption{Грамматика для бесконечного алфавита}
\end{figure}
\end{center}

\subsection{БНФ-нотация для VW-грамматики}
	Существует другая нотация для VW-грамматик, которая иногда используется в теории языков (см., например, Грейбах (Greibach [VW 1974])). Как и следует из названия, такое обозначение берёт свое начало из БНФ-нотации. Пример для рис. 2.17 приведён на рис. 2.19. Гиперобозначения выделены скобками, а терминалы оставлены без изменения.\\
\begin{center}
\begin{figure}[h]
\HSp
\label{figlr-cfcs7}
\begin{tabular}{lcl}
\GrT{$N$} & $\rightarrow$ & $i\ |\ i\ N$\\
\GrT{$A$} & $\rightarrow$ & $a\ |\ b\ |\ c$\\
\GrT{} & $$ & $$\\

\GrT{$<text>_S$} & $\rightarrow$ & $<aN>\ <bN>\ <cN>$\\
\GrT{$<Ai>$} & $\rightarrow$ & $A$\\
\GrT{$<AiN>$} & $\rightarrow$ & $A\ <AN>$\\
\end{tabular}
\caption{Грамматика для бесконечного алфавита}
\end{figure}
\end{center}

\subsection{Аффиксные грамматики}
	Как и VW-грамматики, \textit{аффиксные грамматики} создают глобальные связи путём дублирования информации на раннем этапе. Тем не менее, в них такая информация не является частью нетерминального имени, а передаётся как отдельный параметр - \textsl{аффикс}, который может являться, например, числом. Обычно такие аффиксы передаются членам правила до тех пор, пока они не достигают специальных нетерминалов, называемых \textsl{примитивными предикатами}. Вместо того, чтобы выводить текст, такой нетерминал содержит проверку на действительность. Для того, чтобы правило было действительным, нужно чтобы все его члены были действительными. Аффиксный механизм эквивалентен VW метаобозначениям, но его чуть легче использовать при разборе и чуть труднее использовать при описании грамматики.\\
	Аффиксная грамматика для $a^nb^nc^n$ приведена на рис. 2.20. Две первые строки являются аффиксными определениями $N,M,A,B$. Аффиксу в грамматических правилах обычно предшествует $+$. Имена примитивных предикатов начинаются с $where$. Чтобы вывести $abc$, начнём с $text\ +\ 1$. Это производит:
	$$list\ +\ 1\ +\ a,\ list\ +\ 1\ +\ b,\ list\ +\ 1\ +\ c$$
В свою очередь, второй член выводит:
	$$letter\ +\ b, where\ is\ decreased\ +\ 0\ +\ 1,list\ +\ 0\ +\ b$$
А первый член этого выводит:
	$$where\ is\ +\ b\ +\ b,\ b\ symbol$$
	
Все примитивные предикаты в этом примере истинны, а значит предложение является действительным. Попытка вывести $a\ symbol$ из $letter\ +\ b$ приведёт к тому что приимитивный предикат $where\ is\ +\ a\ +\ b$ вернёт ложь, что сделает такую сентенциальную форму недействительной.\\
	Аффиксные грамматики перестали использовать в пользу атрибутных грамматик, которые очень похожи, но концептуально различны (см. раздел 2.9.1).
% осторожно - может не влезть!!!

\begin{center}
\begin{figure}[h]
\HSp
\label{figlr-cfcs7}
\begin{tabular}{ll}
\GrT{$N,\ M::$} & $integer$\\
\GrT{$A,\ B::$} & $a;\ b;\ c.$\\
\GrT{} & $$ \\

\GrT{$text_S\ +\ N:$} & $list\ +\ N\ +\ a,\ list\ +\ N\ +\ b,\ list\ +\ N\ +\ c.$\\
\GrT{} & $$ \\

\GrT{$list\ +\ N\ +\ A$} & $where\ is\ zero\ +\ N;$\\
\GrT{$$} & $letter\ +\ A,\ where\ is\ decreased\ +\ M\ +\ N,\ list\ +\ M\ +\ A.$\\
\GrT{} & $$ \\

\GrT{$letter\ +\ A$} & $where\ is\ +\ A\ +\ a,\ a\ symbol;$\\
\GrT{$$} & $where\ is\ +\ A\ +\ b,\ b\ symbol;$\\
\GrT{$$} & $where\ is\ +\ A\ +\ c,\ c\ symbol;$\\
\GrT{} & $$ \\

\GrT{$where\ is\ zero\ +\ N:$} & ${N\ =\ 0}.$\\
\GrT{} & $$ \\

\GrT{$where\ is\ decreased\ +\ M\ +\ N:$} & ${M\ =\ N\ -\ 1}.$\\
\GrT{} & $$ \\

\GrT{$where\ is\ +\ A\ +\ B:$} & ${A\ =\ B}.$\\
\end{tabular}
\caption{Аффиксная грамматика для $a^nb^nc^n$}
\end{figure}
\end{center}
	
%31.05.06
\section{Вывод предложений}
\subsection{Общий вид}
	До сих пор мы выводили из наших грамматик лишь одиночные предложения специального вида. Но задачей грамматики является вывод всех возможных \textbf{правильных} предложений. К счастью, существует путь сделать это. Для примера возьмём грамматику $a^nb^nc^n$. Начнём с начального символа и систематически выполним все возможные подстановки, для того, чтобы сгенерировать все сентенциальные формы. Будем ждать, пока не получим что-нибудь \textbf{правильное}, т.е. осмысленное предложение. Попробуйте проделать это вручную для, например, 10 сентенциальных форм. Если мы не будем предельно аккуратны, то натолкнёмся на вывод типа $aSQ,\ aaSQQ,\ aaaSQQQ, \ldots$ и никогда не достигнем конца. Причина в том, что мы слишком сильно заостряем своё внимание на каких-то определённых сентенциальных формах. Нужно не забывать другие, т.е. как бы "`давать им одинаковое количество времени"'. Это можно сделать по следующему алгоритму, который поддерживает очередь (список, в который добавляется в хвост, а удаляется из головы) сентенциальных форм.\\
	Добавим начальный символ в очередь. Далее будем действовать так:
	
\begin{itemize}
	\item Добавим первую сентенциальную форму в очередь.
	\item Просмотри её слева-направо в поиске строк из символов, которые подходят к левой части какого-либо правила.
	\item Для каждой найденной строки сделаем достаточное число копий сентенциальных форм и подставим в каждую из них вместо строки, которая подошла к левой части, другую альтернативу правила. Добавим все сентенциальные формы в очередь.
	\item Если сентенциальная форма не содержит нетерминалов, будем считать её предложением.
	\item Отбросим сентенциальную форму. Она была полностью обработана.
\end{itemize}
	Если не было найдено не одного подходящего правила, и сентенциальная форма  не перешла в законченное предложение, то мы зашли в тупик. Они удаляются автоматически, так что никаких проблем возникнуть не должно.\\
	Приведём первые шаги (рис. 2.21) такого процесса для грамматики с рис. 2.6. Очередь растёт враво, а её головной элемент - левый.\\

\begin{center}
\begin{figure}[h]
\HSp
\label{fig-queue1}
\begin{tabular}{lllr}
\GrT{Step} & Queue & $$ & Result\\
\GrT{$1$} & $S$ & $$ & $$\\
\GrT{$2$} & $abc$ & $aSQ$ & $abc$\\
\GrT{$3$} & $aSQ$ & $$ & $$\\
\GrT{$4$} & $aabcQ$ & $aaSQQ$ & $$\\
\GrT{$5$} & $aaSQQ$ & $aabQc$ & $$\\
\GrT{$6$} & $aabQc$ & $aaabcQQ\ aaaSQQQ$ & $$\\
\GrT{$7$} & $aaabcQQ$ & $aaaSQQQ\ aabbcc$ & $$\\
\GrT{$8$} & $aaaSQQQ$ & $aabbcc\ aaabQcQ$ & $$\\
\GrT{$9$} & $aabbcc$ & $aaabQcQ\ aaaabcQQQ\ aaaaSQQQQ$ & $aabbcc$\\
\GrT{$10$} & $aaabQcQ$ & $aaaabcQQQ\ aaaaSQQQQ$ & $$\\
\GrT{$11$} & $aaaabcQQQ$ & $aaaaSQQQQ\ aaabbccQ\ aaabQQc$ & $$\\
\GrT{$\ldots$} & $ldots$ & $$ & $$
\end{tabular}
\caption{Первые шаги вывода $a^nb^nc^n$}
\end{figure}
\end{center}

Видно, что мы получаем предложение лишь после нескольких шагов. Вообще, "`осмысленные"'  предложения будут появляться всё реже и реже. Причина этого кроется в том, что чем дольше работает процесс, тем больше появляется ответвлений, каждое из которых требует внимания. Тем не менее, можно быть уверенным, что любое предложеие, которое можно вывести, будет выведено. Такой метод называют, по аналогии с поиском, \textit{выводом в-глубину} (breadth-first production). Компьютеры делают это лучше людей.\\

%18/06/06 - p48
Можно решить, что не обязательно заменять \textit{все} левые части самой "`верхней"' сентенциальной формы. Почему бы не заменить лишь первую и дождаться образования сентенциальной формы, а затем обработать вторую? Так сделать не получиться, так как построение первой может разрушить контекст для второй. Вот простой пример:\\
\begin{center}
\begin{figure}[h]
\HSp
\label{fig-exam1}
\begin{tabular}{lcl}
\GrT{$S_S$} & $\rightarrow$ & $AC$\\
\GrT{$A$} & $\rightarrow$ & $b$\\
\GrT{$AC$} & $\rightarrow$ & $ac$\\
\end{tabular}
\end{figure}
\end{center}

Если мы выполним $A \rightarrow\ b$, то это приведёт нас в тупик, а грамматика не выведет ничего. Используя возможные подстановки тоже приведёт нас к тупику, но тем не менее здесь мы уже встретим вторую сентенциальную форму - $ac$. Это является примером грамматики, для которой очередь быстро становится пустой.\\
	Если грамматика является контекстно-свободной, то нас не интересует возможные "`разрушения"' контекста, так что можно спокойно выполнить замену при первом совпадении.\\
	Сделаем два замечания. Во-первых, возможно, что каждая следующая сентенциальная форма не будет содержать нетерминалы. Можно убедиться в этом заранее, проверив грамматику. Конечно же это необязательно, так как можно доказать, что это невозможно. Формальный-лингвист скажет "`Нельзя узнать, производит ли PS грамматика пустое множество"', что означает, что не существует алгоритма, который бы для любой PS-грамматики определял, выводит ли она хотя бы одно предложение. Мы не имеем универсального метода. Мы можем написать программу, которая скажет "`Да"' за конечное время, но которая не сможет ответить "`Нет"' за конечное время. Вообще, наша процедура создания, приведённая выше, является алгоритмом, который может выдать для \textit{конкретной} грамматики корректный "`Да/Нет"' результат (также можно использовать алгоритм, который даёт ответ в виде "`Да/Возможно"') за конечное время. Согласно одной из теорем лингвистики, мы не \textit{всегда} сможем получить ответ, что тем не менее не отменяет возможности извлечь информацию, которая даст приблизительный ответ. Мы увидим, что это рекуррентная проблема. Компютерный инженер боится её, но не запуган запретами формальной лингвистики.\\
%19.06.06
	Во-вторых, предложения образуются в непредсказуемом порядке. Для немонотонных грамматик сентенциальные формы могут сначала "`разрастись"', а потом быстро "`сжаться"', возможно даже до пустой строки. Формальная лингвистика утверждает, что не существует алгоритма, который для любых PS-грамматик будет производить предложения в порядке возрастания (вообще-то, неубывания) их длины.\\
	Вывод всех возможных предложений из VW-грамматики образует специфичную проблему, которая заключается в том, что сравнение идёт с бесконечным множеством левых сторон. См. Грюне (Grune) [VW 1984] для разрешения этой задачи.\\
	\subsection{КС случай}
	Генерировать предложения по КС грамматике намного проще. Тем не менее, опять возможен случай, когда грамматика никогда не произведёт предложение. Но в данном случае, мы сможем узнать об этом заранее. Сначала проверим грамматику на наличие нетерминалов, правая часть которых содержит только терминалы (либо пуста). Эти нетерминалы гарантированно производят что-либо. Далее, найдём все нетерминалы, правая часть которых содержит только терминалы и нетерминалы, которые гарантированно производят что-либо. Это вновь даёт нам нетерминалы, которые производят что-то. Будем повторять до тех пор, пока больше не останется таких нетерминалов. Если нам не повстречается начальный символ, значит он не производит ничего.\\
	Видно, что если это КС грамматика, то можно каждый раз просто заменять самый левый нетерминал (заменить его на все его альтернативы). Конечно, мы можем последовательно заменять самый правый нетерминал. Оба подхода одновременно и схожи, и различны. Для грамматики:
\begin{center}
\begin{figure}[h]
\HSp
\label{fig-exam2}
\begin{tabular}{lrcl}
\GrT{$0.$} & $N$ & $\rightarrow$ & $t\ |\ d\ |\ h$\\
\GrT{$1.$} & $S_S$ & $\rightarrow$ & $N\ |\ L\ \verb|&| \ N$\\
\GrT{$2.$} & $L$ & $\rightarrow$ & $N\ ,\ L\ |\ N$\\
\end{tabular}
\end{figure}
\end{center}

Давайте попробуем разобраться с $d,h \verb|&|h$. Покажем сентенциальные формы для самой левой и правой подстановок, с продукциями и альтернативами. Например, (1b) означает первую продукцию, альтернативу b.
\begin{center}
\begin{figure}[h]
\HSp
\label{fig-exam3}
\begin{tabular}{lclc}
\GrT{$$} & $S$ & $$ $S$\\
\GrT{$1b$} & $$ & $1b$ $$\\
\GrT{$$} & $L\verb|&|N$ & $$ $L\verb|&|N$\\
\GrT{$2a$} & $$ & $0c$ $$\\
\GrT{$$} & $N,L\verb|&|N$ & $$ $L\verb|&|N$\\
\GrT{$0b$} & $$ & $2a$ $$\\
\GrT{$$} & $d,L\verb|&|N$ & $$ $N,L\verb|&|h$\\
\GrT{$2b$} & $$ & $2b$ $$\\
\GrT{$$} & $d,N\verb|&|N$ & $$ $N,N\verb|&|h$\\
\GrT{$0c$} & $$ & $0c$ $$\\
\GrT{$$} & $d,h\verb|&|N$ & $$ $N,h\verb|&|h$\\
\GrT{$0c$} & $$ & $0b$ $$\\
\GrT{$$} & $d,h\verb|&|h$ & $$ $d,h\verb|&|h$\\
\end{tabular}
\end{figure}
\end{center}

Последовательность вывода не так проста, как мы могли бы ожидать. Конечно, применяются те же самые правила и альтернативы, но последовательности и не одинаковые, и не являются зеркальными отражениями друг друга. Тем не менее, оба образуют одинаковое дерево вывода:
%рис.стр40
Если пронуемеровать неетерминалы в порядке их замены (перезаписи), получим разные картины:
% рис.стр40 рис.стр40
Последовательность вывода с заменой самого левого символа называется левосторонним выводом (left-most derication). Мы не должны обозначать правила номерами, это и так естественно для левостороннего вывода. Правосторонний вывод определяется точно так же. Последовательность вывода $S \rightarrow L\verb|&|N \rightarrow N,L\verb|&|N \rightarrow d,L\verb|&|N \rightarrow d,N\verb|&|N \rightarrow d,h\verb|&|N \rightarrow d,h\verb|&|h$ можно обозначить $S\stackrel{*}{\rightarrow}_{l} d,h\verb|&|h$. Аналогично, последовательность $S \rightarrow L\verb|&|N \rightarrow L\verb|&|h \rightarrow N,L\verb|&|h \rightarrow N,N\verb|&|h \rightarrow N,h\verb|&|h \rightarrow d,h\verb|&|h$ можно обозначить как $S\stackrel{*}{\rightarrow}_{r} d,h\verb|&|h$. Вывод любым способом из $S$ предложения $d,h\verb|&|h$ обозначают просто $S\stackrel{*}{\rightarrow} d,h\verb|&|h$.\\
	Задача разбора состоит в построении дерева разбора (или графа) для данной строки, но некоторые (как правило, самые эффективные) методики разбора можно проще понять, если рассматривать разбор как реконструкцию левостороннего (или правостороннего) вывода для входной строки. Дерево разбора строится прямо на ходу. Вот почему название "`[лево/право]-сторонний вывод"' будет часто встречаться в данной книге (обратите внимание на КВ-грамматику, использованную здесь).

%?
\section{Shrink?}
В предыдущих главах мы не всегда могли точно сказать, короче ли правая часть правила его левой части. Нулевой тип грамматик мог иметь различные варианты, монотонные продукции - не имели такого разнообразия, а грамматики 2 и 3 типов могли быть сокращены только выводом пустого символа ($\varepsilon$).\\
	Изначальная иерархия Хомского [Misc 1959] четко определяла, что лишь грамматики нулевого типа могут сокращать сентенциальную форму. Грамматики от 1 до 3 типов являлись монотонными. Более того, тип 1 должен был иметь контекстно-зависимые правила, в которых мог быть замещён лишь один нетеринал в левой части (и не $\varepsilon$). Это образовывало четкую иерархию, в которой каждый предок являлся надмножеством, и в которых все графы вывода (кроме типа 0) были деревьями. В качестве примера вернёмся к языку $a^nb^nc^n$ с рис.2.6:
\begin{center}
\begin{figure}[h]
\HSp
\label{fighopderiv2}
\begin{tabular}{lrcl}
\GrT{$1.$} & $S_S$ & $\rightarrow$ & $abc\ |\ aSQ$\\
\GrT{$2.$} & $bQc$ & $\rightarrow$ & $bbcc$\\
\GrT{$3.$} & $cQ$  & $\rightarrow$ & $Qc$\\
\end{tabular}
\end{figure}
\end{center}
Как мы помним, эта грамматика является монотонной, но не контекстно-зависимой в буквальном смысле. Она может быть преобразована в КЗ расширением третьего правила и введением нового нетерминального символа для $c$:
\begin{center}
\begin{figure}[h]
\HSp
\label{fighopderiv3}
\begin{tabular}{lrcl}
\GrT{$1.$} & $S_S$ & $\rightarrow$ & $abc\ |\ aSQ$\\
\GrT{$2.$} & $bQc$ & $\rightarrow$ & $bbcc$\\
\GrT{$3a.$} & $CQ$  & $\rightarrow$ & $CX$\\
\GrT{$3b.$} & $CX$  & $\rightarrow$ & $QX$\\
\GrT{$3c.$} & $QX$  & $\rightarrow$ & $QC$\\
\GrT{$4.$} & 	$C$  & $\rightarrow$ & $c$\\
\end{tabular}
\end{figure}
\end{center}
Теперь граф продукции (рис. 2.7) превратился в дерево:
%рис. стр42
	Вот ещё одна причина избегать $\varepsilon$-правила: они зметно усложняют и разборщики, и доказательства. Возникает вопрос: почему мы должны вообще использовать $\varepsilon$-правила? Дело в том, что они очень удобны для построения и использования грамматик.\\
	Допустим, имеется язык, описываемый КС грамматикой с $\varepsilon$-правилами и мы желаем получить грамматику без $\varepsilon$-правил. Значит полученная (преобразованная) грамматика \textit{всегда} будет сложнее исходной. Представьте, что имеется система, хранящая информацию в виде предложений следующего вида: "`Амстердам - столица Нидерландов"', "`Трюфели - дорогие"'. Система может отвечать на наши вопросы. Можно так описать её ввод на очень высоком уровне: 	
%битов???
\begin{center}
\begin{figure}[h]
\HSp
\label{fighopderiv4}
\begin{tabular}{ll}
\GrT{$input_S:$} & zero-or-more-bits-of-info\ question
\end{tabular}
\end{figure}
\end{center}
%стр 42
или в расширенной нотации:
\begin{center}
\begin{figure}[h]
\HSp
\label{fighopderiv4}
\begin{tabular}{ll}
\GrT{$input_S:$} & $bit-of-info^{*}\ question$
\end{tabular}
\end{figure}
\end{center}

%20.06.06
Так как \textbf{один-или-несколько-бит-информации} будет помимо всего производить пустую строку, хотя бы одна продукция грамматики должна являться $\varepsilon$-продукцией. Это сразу же видно по символ звездочки ($^*$) в расширенном обозначении. С точки зрения пользователя, данное определение выглядит более чем естественно и полностью подходит для задачи.\\
	Любая попытка описать $\varepsilon$-свободную грамматику для нашей задачи приведёт к объеденению последних из битов информации (\textbf{bit-of-info}) вместе с вопросом (\textbf{question}). Так как вопрос (\textbf{question}) является единственной непустой частью, он должен встречаться во всех продукциях! Теперь это стало довольно громоздким "`сооружением"':

\begin{center}
\begin{figure}[h]
\HSp
\label{fighopderiv4}
\begin{tabular}{ll}
\GrT{$input_S:$} & $question-preceded-by-info$\\
\GrT{$question-preceded-by-info:$} & $question$\\
\GrT{$$} & $| bit-of-info\ question-preceded-by-info$
\end{tabular}
\end{figure}
\end{center}
Чем больше расширяется грамматика, тем больше она работает против нас.\\
	С сугубо теоретической точки зрения это не является проблемой: любой КС язык может быть описан $\varepsilon$-свободной КС грамматикой. Более того, любая $\varepsilon$-содержащая грамматика может быть преобразована в $\varepsilon$-свободную. Мы видели пример такого преобразования выше, а алгоритм более подробно рассматривается в разделе 4.2.3.1. Цена, которую мы заплатили за это, является стандартной для любого преобразования грамматики: это уже не наша исходная грамматика, так что теперь она хуже отражает (или отображает) исходную структуру.\\
%стр 43
	Практики находят $\varepsilon$-продукции очень полезным инструментом, и будет интересно рассмотреть вопрос существования иерархии немонотонных грамматик помимо, известной нам, иерархии Хомского. Грамматики 2 и 3 типа не обязаны быть монотонными (но они всегда могут быть таковыми, если в этом есть нужда). Получается, контекстно-зависимые грамматики с ???-продукциями эквивалентны грамматикам неограниченного нулевого типа, а монотонные грамматики с $\varepsilon$-продукциями эквивалентны грамматикам нулевого типа. Теперь мы владеем достаточным количестовм знаний, чтобы рассмотреть две иерархии в непосредственной близости (рис. 2.22).
%таблица
???
Видно, что разница между немонотонными 0 и 1 типами грамматик незначительна (её вообще нет).\\
	Если, содержащий пустые строки язык, описан грамматикой от 1 до 3 типов, то возникает особый случай. Такая грамматика не входит в монотонную иерархию, так как начальный символ уже имеет длину 1, и никакое правило не может укоротить его. Пустая строка должна быть каким-то образом привязана в качестве особого свойства грамматики. Такой проблемы не возникает в немонотонной иерархии.\\
	Многие методы разбора будут в принципе работать только с $\varepsilon$-свободными грамматиками: порою трудно определить, когда что-то не выводит чего-то. Тем не менее, часто метод разбора может быть слегка изменён, чтобы решить эту задачу.\\

%стр 43
\section{Ограничения КС и КВ грамматик}
Работая с КС грамматикой можно решить, что практически любая задача может быть описана с её помощью. Тем не менее, существуют большое ограничение, которое можно проиллюстрировать с помощью известной теоремы: $uvwxy$.

\subsection{Теорема uvwxy}
	Как только мы получили предложение с помощью КС грамматики, мы можем изучить каждый (терминальный) символ и задать такой вопрос: Как он там оказался? Затем, изучив дерево вывода, можно предположить, что (к примеру) он являлся $n$-ым членом правой части $m$-ой продукции. Левая часть этой продукции, то есть предок нашего символа, являлся, в свою очередь, $p$-ым членом продукции $q$. И так далее, до тех пор, пока не достигнем начального символа. Таким образом можно определить \textit{родословную} символа. Если все пары "`продукция/номер символа"' в родословной символа различны, назовём такой символ - \textit{оригинальным}, а предложение, состоящее только из оригинальных символов так же назовём \textsl{оригинальным}.\\
%???
	Существует лишь конечное число различных возможностей для того, чтобы символ стал оригинальным. Это легко увидеть. Все пары "`продукция/номер символа"' в родословной оригинального символа должны быть различны, так что "`глубина"' его родословной не может превышать число различных пар грамматики. Существует лишь конечное число комбинаций "`продукция/номер символа"'. В теории, число возможных оригинальных родословных для символа может быть очень большим, но на практике оно несущественно: если их больше, например, десяти, то можно считать, что грамматика чрезмерно "`свёрнута"'.\\
	Это образует огромные ограничения для оригинального предложения. Если символ дважды входит в такое предложение, то обе его родословные должны различаться: если бы они были одинаковыми, то предложение не было бы оригинальным. Это означает, что максимальная длина оригинальных предложений ограничена числом всех возможных родословных оригинальных символов. Для среднестатистической грамматики языка программирования это несколько тысяч, т.е. примерно сопостовимо с размерами самой грамматики. Так как существует самое длинное возможное оригинальное предложение, значит число оригинальных предложений так же конечно. Мы подошли к интересному звключению, что любая КС грамматика производит конечное число оригинальных предложений и (возможно) бесконечное число неоригинальных предложений!
%рис 2.23 стр 44
Так что же представляют их себя "`неоригинальные"' предложения? Будет полезно рассмотреть теорему $uvwxy$. Неоригинальные предложения обладают следующим свойством: они содержат в своей родословной хотя бы один повторяющийся символ. Предположим, этот символ - $q$, а повторяющееся правило - $A$. Можно нарисовать рисунок, схожий с рис. 2.23, где часть $w$ выведена самым последним применением продукции $A$, а $uvwxy$ это полное неоригинальное предложение. Теперь можно незамедлительно найти другое неоригинальное предложение с помощью замены самого маленького треугольника (в вершине которого находится $A$) на копию большего треугольника (в вершине которого тоже $A$). См. рис. 2.24.
%рис 2.24 стр 45
	Это новое дерево производит предложение $uvvwxxy$ и легко заметить, что таким образом можно построить полное множество предложений $uv^nwx^ny$ для всех $n \geq 0$. Символ $w$ вложен в одинаковое число "`скобок"' $v$ и $x$, а также в контекст $u$ и $y$.\\
%???
	В итоге, чем длинее становятся предложения КС языка, тем реже встречаются оригинальные. Это и устанавливает в качестве результата теорема $uvwxy$: любое предложение, сгенерированное КС грамматикой, которое длиннее любого возможного оригинального предложения, можно разделить на пять частей: $u,v,w,x,y$ таким образом, что $uv^nwx^ny$ является предложением этого языка для всех $n \geq 0$. Эта теорема существует в различных вариантах. Иногда её называют "`лемма расширения контекстно-свободных языков"'.\\
	Нужно сделать два замечания. Во-первых, если язык продолжает быть оригинальным с увеличением длины предложений, т.е. произведённые предложения не сводятся к многочисленным вложенным структурам, то для такого языка не существует КС грамматики. Мы уже встречали такой контекстно-зависимый язык: $a^nb^nc^n$. Легко видеть (но не так легко доказать!), что с ростом длины предложения, не существует никаких вложенных предложений. Соответсвенно, для такого языка не существует КС грамматики.\\
	Во-вторых, максимальная длина оригинального предложения является свойством грамматики, а не языка. Создавая более сложные языки можно увеличить число оригинальных предложений и раздвинуть ограничение из-за которых мы вынуждены прибегать к вложенности. Если мы создадим бесконечно сложную грамматику, то отодвинем эту границу в бесконечность, а значит получим PS язык. Как сделать грамматику бесконечно сложной можно узнать из раздела 2.4.

%21.06.06
\subsection{Теорема uvw}
%рис 2.25 стр 46
	Для языков третьего типа применяется упрощенная форма теоремы $uvwxy$. Мы видели, что все сентенциальные формы для КВ-грамматики (FS) содержат лишь один нетерминал, который находится в конце. В течении вывода чрезмерно длинного предложения, один или несколько нетерминалов могут встречаться несколько раз, так как существует лишь их конечное множество. На рисунке 2.25 показано несколько сентенциальных форм. Подстрока $v$ была выведена первым $A$ и следующим, $u$ является последовательностью, которая позволяет нам достичь $A$. $w$ является последовательностью, которая позволяет нам завершить вывод. Начиная со второго $A$, мы могли бы идти тем же путём, что и после первого. В результате получили бы $uvvw$. Это приводит нас к теореме $uvw$, называемой "`леммой расширения регулярных языков"' - любая достаточно длинная последовательность (строка) регулярного языка может быть разделена на три части: $u,v,w$, таким образом, что $uv^nw$ является предложением этого языка для всех $n \geq 0$.\\


\section{Очищаем грамматику}
	Помимо ограничения (в грамматике) на наличие лишь одного нетерминала в левых частях продукций, существует несколько условий, от которых может страдать грамматика.\\

\subsection{Неопределённые нетерминалы}
Правые части некоторых продукций могут содержать нетерминалы, для которых не определены правила. Вообще, это не сильно ограничивает процесс вывода, описанный в разделе 2.5.2: сентенциальная форма, участвующая в левостороннем выводе, содержащая неопределённый нетерминал (не будет найдено нужного совпадения) приведёт в тупик. Продукция, содержащая в правой части неопределённый нетерминал никогда не встретится, а значит может быть исключена из грамматики. Сделав это, мы, безусловно, исключим последнее определение другого нетерминал, который в свою очередь станет неопределённм, и т.д.\\
	С теоретической точки зрения в этом нет ничего странного, но на практике грамматика, содержащая неопределённый нетерминал является "`ошибочной"'. Любое средство проверки грамматик должно найти ошибку.
	
\subsection{Неиспользуемые нетерминалы}
	Если нетерминал никогда не встречается в правой части продукций, его правило никогда не понадобится. Конечно, это означает всего-лишь, что грамматика составлена неверно.\\
	Такую ошибку труднее обнаружить, чем кажется. Проверки всех правых частей не достаточно: представьте продукцию $X \rightarrow aX$. Пусть символ $X$ более нигде не встречается. Хотя он и находится в правой части, он никогда не будет использован. Алгоритм для нахождения множества неиспользованных нетерминалов приведён в разделе 4.2.3.4.

\subsection {Невыводящие нетерминалы}
%???
	Представьте, что $X$ имеет единственную продукцию $X \rightarrow aX$ и является доступным из начального символа. Данный символ не влияет каким-либо образом на предложения языка, но мы не можем от него избавиться, так как он используется в грамматике. Любой нетерминал, который не образует собственного языка (т.н."`подъязыка"') является непродуктивным, а его продукции могут быть удалены. Обратите внимание на то, что это делает нетерминал неопределённым. Алгоритм нахождения невыводящих нетерминалов приведён в разделе 4.2.3.3.\\
	Для того, чтобы привести грамматику в должный вид, необходимо сначал удалить невыводящие нетерминалы, затем - неопределённые, а самыми последними - неиспользуемые. Три данных группы символов называют \textit{бесполезными нетерминалами}.

\subsection{Циклы}
%??? неявными
	Вышеприведённое определение называет "`полезными"' все правила, которые принимают участие в процессе генерации предложений, но сущесвует особый класс правил, которые не совсем полезны: это правила вида $A \rightarrow A$. Они называются \textit{циклами}. Циклы иногда являются неявными: $A \rightarrow B, B \rightarrow C, C \rightarrow A$. При выводе предложения может образоваться цикл, но в любом случае должен существовать нецикличный вывод данного предложения. Любое предложение, использующее при выводе цикл, называют \textit{бесконечно неоднозначным}, что означает наличие бесконечного множества деревьев вывода. Алгоритм нахождения циклов приведён в разделе 4.1.2.\\
	Различные разборщики по-разному обрабатывают циклы. Тогда как некоторые (большинство) пытаются построить бесконечное количество деревьев разбора, другие (например, CYK разборщики) сворачивают цикл, как описано выше. Самые "`детерминированные"' разборщики просто выдают ошибку и отказываются работать с такой грамматикой. Сложность работы с циклами возрастает из-за того, что цикл может быть скрыт за $\varepsilon$-правилом: цикл может быть замечен только если нетерминал вывел $\varepsilon$.

\section{Семантические связи}
	Иногда разбор служит только для проверки валидности введённой строки. Нам необходимо знать лишь, принадлежит ли она к данному языку. Мы уверены, что грамматика верно описывает язык. Тем не менее, часто нужно ещё кое-что: мы знаем, что строка несёт определённую информацию, имеет смысл. Её семантика привязана к структуре дерева вывода (если это не так - у нас неправильная грамматика).\\
	Привязка семантики к грамматике - довольно простой процесс: к каждой продукции прикрепляется так называемое \textit{семантическое выражение}, которое связывает семантику членов правой части продукции с семантикой всей продукции (в этом случае семантическая информация передаётся вверх к корню по дереву вывода). Возможно и противоположное направление(в котором семантическая информация передаётся вниз к листьям по дереву вывода) или смешанное (семантическая информация передаётся вверх и вниз по дереву вывода до тех пор, пока не будет достигнуто стабильное состояние). Семантическая информация передаваемая вниз называется \textit{наследуемой}: каждая продукция наследует её от своего предка; семантическая информация передаваемая вверх называется \textit{получаемой}: каждая продукция получает её от своих потомков.\\
% 24.06.06
	Существует много разнообразных способов описать семантические связи. Так как мы имеем дело с разбором, а значит чаще с синтаксисом, а не семантикой, приведём всего лишь два наиболее распространеённых способа: атрибутные и трансдукционные грамматики. Будем использовать один пример: язык, описывающий суммы цифр. Семантикой предложений является значение суммы. Грамматика приведена на рис. 2.26.
\begin{center}
\begin{figure}[h]
\HSp
\label{fig226}
\begin{tabular}{rcl}
\GrT{$Sum_S$} & $\rightarrow$ & $Digit$\\
\GrT{$Sum$} & $\rightarrow$ & $Sum + Digit$\\
\GrT{$Digit$} & $\rightarrow$ & $0\ |\ 1\ | \ldots\ |\ 9$
\end{tabular}
\caption{Грамматика для описания сумм из еденичных символов}
\end{figure}
\end{center}
Приведём одно из возможных предложений языка: $3+5+1$. Его семантика равна 9.

\subsection{Атрибутные грамматики}
Семантические выражения в атрибутных грамматиках подразумевают, что каждый узел дерева вывода имеет место хотя бы для одного \textit{атрибута}, который является обыкновенным значением (числами, строками и т.д.), хранящимся в узле.  Для простоты ограничимся одним возможным атрибутом в узле. Семантическое выражение является формулой, которая вычисляет значение нетерминалов продукции (узлов дерева вывода) из значений других нетерминалов этой же продукции.\\
	Если семантическое выражение правила $R$ вычисляет атрибут левосторонней части $R$, то такое выражение называют \textit{производным}; если оно вычисляет атрибут одного из нетерминалов правой части $R$, например $T$, то такой атрибут называют \textit{наследуемым}. Производные атрибуты часто называют "`синтезированными"'. Атрибутная грамматика для нашего примера приведена далее:
\begin{center}
\begin{figure}[h]
\HSp
\label{figattrib}
\begin{tabular}{rclll}
\GrT{$1.$} & $Sum_S$ & $\rightarrow$ & $Digit$ & $\left\{ A_0 := A_1 \right\}$\\
\GrT{$2.$} & $Sum$ & $\rightarrow$ & $Sum + Digit$ & $\left\{ A_0 := A_1  + A_3 \right\}$\\
\GrT{$3a.$} & $Digit$ & $\rightarrow$ & $0$ & $\left\{ A_0 := 0 \right\}$\\
\GrT{$$} & $\ldots$ & $\ldots$ & $$\\
\GrT{$3j.$} & $Digit$ & $\rightarrow$ & $9$ & $\left\{ A_0 := 9 \right\}$\\
\end{tabular}
\end{figure}
\end{center}
Семантические выражения заключены в фигурные скобки. $A_0$ является (производным) атрибутом для левой стороны, $A_1 \cdots A_n$ являются атрибутами элементами правой части. Обычно, терминалы в правой части не имеют атрибутов; $Digit$ в правиле 2 находится в позиции 3, а его атрибутом является $A_3$. Большинство систем обработки атрибутных грамматик имеют улучшенные средства для описания правил $3a \cdots 3j$.\\
%рис. 2.27 стр 49
	Первичное дерево разбора $3+5+1$ показано на рис. 2.27. Сначала известны атрибуты лишь листьев, но как только станут известны все атрибуты правых сторон дерева, мы сможем использовать их семантику для вычисления атрибутов из левых сторон. Таким образом, значения атрибутов "`проталкиваются"' вверх к начальному символу и приводят к вычислению семантики всего предложения. См. рис. 2.28. Атрибутные грамматики являются очень эффективным средством обработки семантики языков.
%рис. 2.28 стр 49

%проверенно:
%25.06.06
\subsection{Трансдукционные грамматики}
\textit{Трансдукционная грамматика} определяет семантику строки ("`входной строки"') через другую строку, "`выходную строку"' или "`преобразование"',  а не через последний атрибут (т.е. атрибут начального символа). Этот метод не такой мощный, но намного более простой, нежели использующий атрибуты. Семантическое выражение для продукции содержит лишь строку,  которая служит выходом соответсвующего узла. Это подразумевает, что такая строка для конкретного узла должна "`выводиться"' после "`вывода"' строк для всех её потомков. Возможны и другие варианты. Приведём пример: 
\begin{center}
\begin{figure}[h]
\HSp
\label{figattrib2}
\begin{tabular}{rclll}
\GrT{$1.$} & $Sum_S$ & $\rightarrow$ & $Digit$ & $\left\{ make it the result \right\}$\\
\GrT{$2.$} & $Sum$ & $\rightarrow$ & $Sum + Digit$ & $\left\{ add it to the previous result \right\}$\\
\GrT{$3a.$} & $Digit$ & $\rightarrow$ & $0$ & $\left\{ take a 0 \right\}$\\
\GrT{$$} & $\ldots$ & $\ldots$ & $$\\
\GrT{$3j.$} & $Digit$ & $\rightarrow$ & $9$ & $\left\{ take a 9 \right\}$\\
\end{tabular}
\end{figure}
\end{center}


Трансдукционная грамматика преобразует $3+5+1$ в следующую последовательность "`комманд"':\\
\begin{itemize}
	\item взять 3 (take a 3)
	\item подсчитать результат (make it the result)
	\item взять 5 (take a 5)
	\item добавить к предыдуще вычесленному результату (add it to the previous result)
	\item взять 1 (take a 1)
	\item добавить к предыдуще вычесленному результату (add it to the previous result)
\end{itemize}

что и является по сути "`значением"' предложения $3+5+1$.

%??? название
\section{Метафоричное сравнение грамматик}
В данной книге нередко встречаются фразы следующего вида: "`Грамматика типа $n$ мощнее грамматики типа $n+1$, для $n=0,1,2$"', "`регулярная грамматика (3 тип) не обладает достаточной мощью для описания скобочных выражений"'. Хотелось бы знать, что же кроется за понятием "`мощь"'. Кто-то может посчитать, что подразумевается возможность генерировать все большие и большие множества. Это не верно, так как наиболее большое множество строк $\sum^*$ можно легко получить с помощью грамматики 3 типа:
$$ S_S \rightarrow [\sum ] S\ |\ \varepsilon$$
,где $[\sum]$ представляет символы языка. Более мощная грамматика может точнее очертить границу между корректным и некорректным предложенями. Некоторые границы настолько расплывчаты, что не могут быть описаны ни одной грамматикой (т.е. не могут быть получены никаким процессом генерации).\\
	На рис. 2.29 изображена роза, которая отражает разницу между грамматиками. Сама роза изображает язык (представьте, что предложения языка являются молкулами, из которых состоит цветок); грамматики изображает искуственный абрис. Регулярная грамматика позволяет использовать лишь прямые линии, что приводит к плохому результату. КС грамматика позволяет использовать наклонные линии и круги. Результат уже лучше. КЗ грамматика ещё точнее описывает розу, но линии все ещё слишком гладкие для того, чтобы очертить резкие штрихи. С помощью неограниченной PS грамматики можно приблизиться к идеально нарисованной розе. Тем не менее, розу никак нельзя нарисовать точно такой же, каковой она приведена на нашем рисунке (необходима бесконечная точность).\\	
%стр 50
	Можно привести и более "`жизненный"' пример - сравнить множество программ, написанных на Паскале\footnote{Паскаль приведён в пример, так как он скорее всего знаком читателю. Сойдёт любой язык программирования, для которого существует КС грамматика}, которые могут быть сгенерированны различными грамматиками.\\
	
\begin{itemize}
	\item Множество лексически корректных программ (Паскаль) может быть сгенерированно регулярной грамматикой. Программа на Паскале является лексически корректной, если все константы находятся в надлежащей форме, блок комментариев закрывается до конца файла, и т.д.
	\item Множество всех синтаксически корректных программ (Паскаль) может быть сгенерированно КС грамматикой. Эти программы написаны в соответствии с КС грамматикой языка. 
	\item Множество всех семантически корректных программ (Паскаль)может быть сгенерированно КЗ грамматикой (или более удобной VW грамматикой). Эти программы успешно компилируются.
	\item Множество всех программ (Паскаль), которые выполняются за конечное время, могуть быть описаны с помощью PS грамматики. Такая грамматика будет слишком сложной (даже если она находится в форме ван Вингардена), так как она вынуждена описывать все процедуры и систему исполнения (run-time system).
	\item Множество всех программ (Паскаль), решающих определённую задачу (например, которые играют с пользователем в шахматы) не может быть описано грамматикой (не смотря на то, что множество конечно)
\end{itemize}

Обратите внимание, что каждое последующее множество является подмножеством предыдущего.
%рис  